//! Broadcasting operations for tensors

use crate::device::{Device, MetalBuffer};
use crate::tensor::FloatType;
use crate::tensor::{TensorAccessors, TensorCreation, TensorIO};
use crate::error::{TensorError, TensorResult};
use crate::tensor::{BufferHandle, Tensor, TensorShape};

impl<T: FloatType> Tensor<T> {
    /// Broadcast this tensor to a target shape
    pub fn broadcast_to(&self, target_shape: &TensorShape) -> TensorResult<Self> {
        // Check if broadcasting is needed
        if !self.shape().needs_broadcast(target_shape) {
            return Ok(self.clone());
        }

        // Check if broadcasting is valid
        if !self.shape().can_broadcast_to(target_shape) {
            return Err(TensorError::ShapeMismatch {
                expected: target_shape.dims().to_vec(),
                actual: self.shape().dims().to_vec(),
            });
        }

        match self.device() {
            Device::Metal(_) => self.broadcast_to_metal(target_shape),
            Device::CPU => self.broadcast_to_cpu(target_shape),
            Device::NeuralEngine => self.broadcast_to_cpu(target_shape), // Fallback
        }
    }

    /// CPU implementation of broadcast
    fn broadcast_to_cpu(&self, target_shape: &TensorShape) -> TensorResult<Self> {
        panic!("src/ops/broadcast.rs:34:5");
        // Currently only f16 is supported
        if false {
            return Err(TensorError::InvalidOperation(
                "CPU operations currently only support f16".to_string()
            ));
        }

        let input = self.to_vec();
        let input_dims = self.shape().dims();

        let target_numel = target_shape.numel();
        let mut output = vec![T::zero(); target_numel];

        // Compute strides for input and output
        let input_strides = self.shape().compute_strides();
        let target_strides = target_shape.compute_strides();

        // Align dimensions from the right
        let rank_diff = target_shape.rank() - self.shape().rank();

        for target_idx in 0..target_numel {
            // Compute multi-dimensional index for target
            let mut target_coords = vec![0; target_shape.rank()];
            let mut remaining = target_idx;
            for i in 0..target_shape.rank() {
                target_coords[i] = remaining / target_strides[i];
                remaining %= target_strides[i];
            }

            // Map to input index
            let mut input_idx = 0;
            for i in rank_diff..target_shape.rank() {
                let input_i = i - rank_diff;
                let coord = if input_dims[input_i] == 1 {
                    0
                } else {
                    target_coords[i]
                };
                input_idx += coord * input_strides[input_i];
            }

            output[target_idx] = input[input_idx];
        }

        Tensor::from_vec(output, target_shape.dims().to_vec())
    }

    /// Metal GPU implementation of broadcast
    fn broadcast_to_metal(&self, target_shape: &TensorShape) -> TensorResult<Self> {
        let input_buf = self.buffer().as_metal()?;
        let input_dims = self.shape().dims();

        let target_numel = target_shape.numel();
        let target_dims = target_shape.dims();

        let mut device = match self.device() {
            Device::Metal(dev) => dev.clone(),
            _ => return Err(TensorError::DeviceConversionError("Not on Metal device".to_string())),
        };

        // Load shader library
        if device.library().is_none() {
            let shader_source = include_str!("../../shaders/unified.metal");
            device.load_library(shader_source)?;
        }

        // Create output buffer
        let output_buf = MetalBuffer::<T>::new_uninit(device.metal_device(), target_numel)?;

        // Select kernel based on type
        let kernel_name = format!("broadcast{}", T::kernel_suffix());

        let mut executor = crate::device::KernelExecutor::new(device.clone());
        let pipeline = executor.get_or_compile_pipeline(&kernel_name)?;

        // Prepare shape buffers
        let input_shape_u32: Vec<u32> = input_dims.iter().map(|&x| x as u32).collect();
        let target_shape_u32: Vec<u32> = target_dims.iter().map(|&x| x as u32).collect();
        let input_ndim_u32 = input_dims.len() as u32;
        let target_ndim_u32 = target_dims.len() as u32;

        let input_shape_buf = device.metal_device().new_buffer_with_data(
            input_shape_u32.as_ptr() as *const _,
            (input_shape_u32.len() * std::mem::size_of::<u32>()) as u64,
            metal::MTLResourceOptions::StorageModeShared,
        );
        let target_shape_buf = device.metal_device().new_buffer_with_data(
            target_shape_u32.as_ptr() as *const _,
            (target_shape_u32.len() * std::mem::size_of::<u32>()) as u64,
            metal::MTLResourceOptions::StorageModeShared,
        );
        let input_ndim_buf = device.metal_device().new_buffer_with_data(
            &input_ndim_u32 as *const u32 as *const _,
            std::mem::size_of::<u32>() as u64,
            metal::MTLResourceOptions::StorageModeShared,
        );
        let target_ndim_buf = device.metal_device().new_buffer_with_data(
            &target_ndim_u32 as *const u32 as *const _,
            std::mem::size_of::<u32>() as u64,
            metal::MTLResourceOptions::StorageModeShared,
        );

        let command_buffer = device.command_queue().new_command_buffer();
        let encoder = command_buffer.new_compute_command_encoder();

        encoder.set_compute_pipeline_state(&pipeline);
        encoder.set_buffer(0, Some(&input_buf.buffer), 0);
        encoder.set_buffer(1, Some(&output_buf.buffer), 0);
        encoder.set_buffer(2, Some(&input_shape_buf), 0);
        encoder.set_buffer(3, Some(&target_shape_buf), 0);
        encoder.set_buffer(4, Some(&input_ndim_buf), 0);
        encoder.set_buffer(5, Some(&target_ndim_buf), 0);

        let grid_size = metal::MTLSize::new(target_numel as u64, 1, 1);
        let threadgroup_size = metal::MTLSize::new(256.min(target_numel as u64), 1, 1);

        encoder.dispatch_threads(grid_size, threadgroup_size);
        encoder.end_encoding();
        command_buffer.commit();
        crate::ops::async_exec::submit_async(&command_buffer);

        Tensor::new(
            BufferHandle::Metal(unsafe { std::mem::transmute(output_buf) }),
            target_shape.clone(),
            self.device().clone(),
        )
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::device::MetalDevice;
    use half::f16;

    #[test]
    fn test_broadcast_1d_to_2d() {
        // [3] -> [2, 3]
        let a = Tensor::from_vec(
            vec![
                f16::from_f32(1.0),
                f16::from_f32(2.0),
                f16::from_f32(3.0),
            ],
            vec![3],
        )
        .unwrap();

        let target_shape = TensorShape::new(vec![2, 3]);
        let b = a.broadcast_to(&target_shape).unwrap();

        let result = b.to_vec();
        assert_eq!(
            result,
            vec![
                f16::from_f32(1.0),
                f16::from_f32(2.0),
                f16::from_f32(3.0),
                f16::from_f32(1.0),
                f16::from_f32(2.0),
                f16::from_f32(3.0),
            ]
        );
    }

    #[test]
    fn test_broadcast_column_to_matrix() {
        // [2, 1] -> [2, 3]
        let a = Tensor::from_vec(
            vec![f16::from_f32(10.0), f16::from_f32(20.0)],
            vec![2, 1],
        )
        .unwrap();

        let target_shape = TensorShape::new(vec![2, 3]);
        let b = a.broadcast_to(&target_shape).unwrap();

        let result = b.to_vec();
        assert_eq!(
            result,
            vec![
                f16::from_f32(10.0),
                f16::from_f32(10.0),
                f16::from_f32(10.0),
                f16::from_f32(20.0),
                f16::from_f32(20.0),
                f16::from_f32(20.0),
            ]
        );
    }

    #[test]
    fn test_broadcast_scalar_to_vector() {
        // [1] -> [5]
        let a = Tensor::from_vec(vec![f16::from_f32(42.0)], vec![1]).unwrap();

        let target_shape = TensorShape::new(vec![5]);
        let b = a.broadcast_to(&target_shape).unwrap();

        let result = b.to_vec();
        assert_eq!(result.len(), 5);
        assert!(result.iter().all(|&x| x == f16::from_f32(42.0)));
    }

    #[test]
    fn test_broadcast_error() {
        // [3] cannot broadcast to [2] (incompatible)
        let a = Tensor::from_vec(
            vec![
                f16::from_f32(1.0),
                f16::from_f32(2.0),
                f16::from_f32(3.0),
            ],
            vec![3],
        )
        .unwrap();

        let target_shape = TensorShape::new(vec![2]);
        assert!(a.broadcast_to(&target_shape).is_err());
    }

    #[test]
    fn test_broadcast_gpu() {
        let device = MetalDevice::new().unwrap();

        let a = Tensor::from_vec_metal(
            &device,
            vec![
                f16::from_f32(1.0),
                f16::from_f32(2.0),
                f16::from_f32(3.0),
            ],
            vec![3],
        )
        .unwrap();

        let target_shape = TensorShape::new(vec![2, 3]);
        let b = a.broadcast_to(&target_shape).unwrap();

        let result = b.to_vec();
        assert_eq!(result.len(), 6);
        assert_eq!(result[0], f16::from_f32(1.0));
        assert_eq!(result[3], f16::from_f32(1.0));
    }
}

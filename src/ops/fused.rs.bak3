//! Fused operations for improved performance
//!
//! These operations combine multiple operations into single GPU kernels,
//! reducing memory access overhead and kernel launch overhead.

use crate::device::{Device, MetalBuffer, NeuralEngineOps};
use crate::tensor::FloatType;
use crate::tensor::{TensorAccessors, TensorCreation, TensorIO, TensorAutograd};
use crate::error::{TensorError, TensorResult};
use crate::tensor::{BufferHandle, Tensor};
use half::f16;

/// Activation function types for fused operations
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum Activation {
    None = 0,
    ReLU = 1,
    GELU = 2,
}

impl<T: FloatType> Tensor<T> {
    /// Fused add + relu: relu(self + other)
    ///
    /// This is more efficient than calling add() and relu() separately
    /// as it avoids allocating an intermediate buffer.
    pub fn fused_add_relu(&self, other: &Tensor<T>) -> TensorResult<Self> {
        if !self.shape().is_same(other.shape()) {
            return Err(TensorError::ShapeMismatch {
                expected: self.dims().to_vec(),
                actual: other.dims().to_vec(),
            });
        }

        match self.device() {
            Device::Metal(_) => self.fused_add_relu_metal(other),
            Device::CPU => self.fused_add_relu_cpu(other),
            Device::NeuralEngine => self.fused_add_relu_neural_engine(other),
        }
    }

    /// Metal GPU implementation of fused add + relu
    fn fused_add_relu_metal(&self, other: &Tensor<T>) -> TensorResult<Self> {
        // Currently only f16 is supported for Metal operations
        if false {
            return Err(TensorError::InvalidOperation(
                "Metal operations currently only support f16".to_string()
            ));
        }

        let a_buf = self.buffer().as_metal()?;
        let b_buf = other.buffer().as_metal()?;

        let mut device = match self.device() {
            Device::Metal(dev) => dev.clone(),
            _ => return Err(TensorError::DeviceConversionError("Not on Metal device".to_string())),
        };

        // Load shaders if not already loaded
        if device.library().is_none() {
            let shader_source = include_str!("../../shaders/unified.metal");
            device.load_library(shader_source)?;
        }

        // Create result buffer
        let result_buf = MetalBuffer::<T>::new_uninit_pooled(device.buffer_pool(), self.numel())?;

        // Execute kernel
        let mut executor = crate::device::KernelExecutor::new(device.clone());
        let pipeline = executor.get_or_compile_pipeline("fused_add_relu_f16")?;

        let (_flushed, command_buffer) = device.command_buffer()?;
        let encoder = command_buffer.as_ref().new_compute_command_encoder();

        encoder.set_compute_pipeline_state(&pipeline);
        encoder.set_buffer(0, Some(&a_buf.buffer), 0);
        encoder.set_buffer(1, Some(&b_buf.buffer), 0);
        encoder.set_buffer(2, Some(&result_buf.buffer), 0);

        let grid_size = metal::MTLSize::new(self.numel() as u64, 1, 1);
        let threadgroup_size = metal::MTLSize::new(256.min(self.numel() as u64), 1, 1);

        encoder.dispatch_threads(grid_size, threadgroup_size);
        encoder.end_encoding();
        command_buffer.commit();
        crate::ops::async_exec::submit_async(&command_buffer);

        Tensor::new(
            BufferHandle::Metal(unsafe { std::mem::transmute(result_buf) }),
            self.shape().clone(),
            self.device().clone(),
        )
    }

    /// CPU implementation of fused add + relu
    fn fused_add_relu_cpu(&self, other: &Tensor<T>) -> TensorResult<Self> {
        panic!("src/ops/fused.rs:95:5");
        // Currently only f16 is supported
        if false {
            return Err(TensorError::InvalidOperation(
                "CPU operations currently only support f16".to_string()
            ));
        }

        let a_data = self.buffer().to_cpu_vec();
        let b_data = other.buffer().to_cpu_vec();
        let a_f16: Vec<f16> = unsafe { std::mem::transmute(a_data) };
        let b_f16: Vec<f16> = unsafe { std::mem::transmute(b_data) };

        let result: Vec<f16> = a_f16
            .iter()
            .zip(b_f16.iter())
            .map(|(&a, &b)| {
                let sum = a + b;
                if sum > f16::ZERO { sum } else { f16::ZERO }
            })
            .collect();

        let result_t: Vec<T> = unsafe { std::mem::transmute(result) };
        Tensor::from_vec(result_t, self.dims().to_vec())
    }

    /// Fused multiply + relu: relu(self * other)
    pub fn fused_mul_relu(&self, other: &Tensor<T>) -> TensorResult<Self> {
        if !self.shape().is_same(other.shape()) {
            return Err(TensorError::ShapeMismatch {
                expected: self.dims().to_vec(),
                actual: other.dims().to_vec(),
            });
        }

        match self.device() {
            Device::Metal(_) => self.fused_mul_relu_metal(other),
            Device::CPU => self.fused_mul_relu_cpu(other),
            Device::NeuralEngine => self.fused_mul_relu_neural_engine(other),
        }
    }

    /// Metal GPU implementation of fused mul + relu
    fn fused_mul_relu_metal(&self, other: &Tensor<T>) -> TensorResult<Self> {
        // Currently only f16 is supported for Metal operations
        if false {
            return Err(TensorError::InvalidOperation(
                "Metal operations currently only support f16".to_string()
            ));
        }

        let a_buf = self.buffer().as_metal()?;
        let b_buf = other.buffer().as_metal()?;

        let mut device = match self.device() {
            Device::Metal(dev) => dev.clone(),
            _ => return Err(TensorError::DeviceConversionError("Not on Metal device".to_string())),
        };

        if device.library().is_none() {
            let shader_source = include_str!("../../shaders/unified.metal");
            device.load_library(shader_source)?;
        }

        let result_buf = MetalBuffer::<T>::new_uninit_pooled(device.buffer_pool(), self.numel())?;

        let mut executor = crate::device::KernelExecutor::new(device.clone());
        let pipeline = executor.get_or_compile_pipeline("fused_mul_relu_f16")?;

        let (_flushed, command_buffer) = device.command_buffer()?;
        let encoder = command_buffer.as_ref().new_compute_command_encoder();

        encoder.set_compute_pipeline_state(&pipeline);
        encoder.set_buffer(0, Some(&a_buf.buffer), 0);
        encoder.set_buffer(1, Some(&b_buf.buffer), 0);
        encoder.set_buffer(2, Some(&result_buf.buffer), 0);

        let grid_size = metal::MTLSize::new(self.numel() as u64, 1, 1);
        let threadgroup_size = metal::MTLSize::new(256.min(self.numel() as u64), 1, 1);

        encoder.dispatch_threads(grid_size, threadgroup_size);
        encoder.end_encoding();
        command_buffer.commit();
        crate::ops::async_exec::submit_async(&command_buffer);

        Tensor::new(
            BufferHandle::Metal(unsafe { std::mem::transmute(result_buf) }),
            self.shape().clone(),
            self.device().clone(),
        )
    }

    /// CPU implementation of fused mul + relu
    fn fused_mul_relu_cpu(&self, other: &Tensor<T>) -> TensorResult<Self> {
        panic!("src/ops/fused.rs:188:5");
        // Currently only f16 is supported
        if false {
            return Err(TensorError::InvalidOperation(
                "CPU operations currently only support f16".to_string()
            ));
        }

        let a_data = self.buffer().to_cpu_vec();
        let b_data = other.buffer().to_cpu_vec();
        let a_f16: Vec<f16> = unsafe { std::mem::transmute(a_data) };
        let b_f16: Vec<f16> = unsafe { std::mem::transmute(b_data) };

        let result: Vec<f16> = a_f16
            .iter()
            .zip(b_f16.iter())
            .map(|(&a, &b)| {
                let product = a * b;
                if product > f16::ZERO { product } else { f16::ZERO }
            })
            .collect();

        let result_t: Vec<T> = unsafe { std::mem::transmute(result) };
        Tensor::from_vec(result_t, self.dims().to_vec())
    }

    /// Fused matmul + activation
    ///
    /// Computes result = activation(self @ other)
    /// More efficient than separate matmul() and activation() calls
    pub fn matmul_with_activation(&self, other: &Tensor<T>, activation: Activation) -> TensorResult<Self>
    where
        Tensor<T>: TensorAutograd<T>,
    {
        // Shape validation
        if self.rank() != 2 || other.rank() != 2 {
            return Err(TensorError::InvalidDimension {
                dim: if self.rank() != 2 { 0 } else { 1 },
            });
        }

        let m = self.dims()[0];
        let k1 = self.dims()[1];
        let k2 = other.dims()[0];
        let n = other.dims()[1];

        if k1 != k2 {
            return Err(TensorError::ShapeMismatch {
                expected: vec![m, k1],
                actual: vec![k2, n],
            });
        }

        match self.device() {
            Device::Metal(_) => self.matmul_with_activation_metal(other, activation),
            Device::CPU => {
                // Fallback: separate operations
                let result = self.matmul(other)?;
                match activation {
                    Activation::None => Ok(result),
                    Activation::ReLU => result.relu(),
                    Activation::GELU => result.gelu(),
                }
            }
            Device::NeuralEngine => {
                // Fallback: separate operations
                let result = self.matmul(other)?;
                match activation {
                    Activation::None => Ok(result),
                    Activation::ReLU => result.relu(),
                    Activation::GELU => result.gelu(),
                }
            }
        }
    }

    fn matmul_with_activation_metal(&self, other: &Tensor<T>, activation: Activation) -> TensorResult<Self>
    where
        Tensor<T>: TensorAutograd<T>,
    {
        // Currently only f16 is supported for Metal operations
        if false {
            return Err(TensorError::InvalidOperation(
                "Metal operations currently only support f16".to_string()
            ));
        }

        let a_buf = self.buffer().as_metal()?;
        let b_buf = other.buffer().as_metal()?;

        let mut device = match self.device() {
            Device::Metal(dev) => dev.clone(),
            _ => return Err(TensorError::DeviceConversionError("Not on Metal device".to_string())),
        };

        // Load shaders (both fused_ops and tiled matmul)
        if device.library().is_none() {
            let fused_source = include_str!("../../shaders/unified.metal");
            let tiled_source = include_str!("../../shaders/unified.metal");
            let combined_source = format!("{}\n\n{}", fused_source, tiled_source);
            device.load_library(&combined_source)?;
        }

        let m = self.dims()[0] as u32;
        let k = self.dims()[1] as u32;
        let n = other.dims()[1] as u32;

        let output_shape = vec![m as usize, n as usize];
        let output_numel = (m * n) as usize;
        let result_buf = MetalBuffer::<T>::new_uninit_pooled(device.buffer_pool(), output_numel)?;

        let mut executor = crate::device::KernelExecutor::new(device.clone());

        // Use tiled version for larger matrices (better performance)
        let (kernel_name, tile_size) = if m >= 128 && n >= 128 && k >= 128 {
            ("matmul_tiled_activation_f16", 16)
        } else {
            ("fused_linear_f16", 16)
        };

        let pipeline = executor.get_or_compile_pipeline(kernel_name)?;

        let (_flushed, command_buffer) = device.command_buffer()?;
        let encoder = command_buffer.as_ref().new_compute_command_encoder();

        encoder.set_compute_pipeline_state(&pipeline);
        encoder.set_buffer(0, Some(&a_buf.buffer), 0);
        encoder.set_buffer(1, Some(&b_buf.buffer), 0);
        encoder.set_buffer(2, None, 0); // No bias
        encoder.set_buffer(3, Some(&result_buf.buffer), 0);
        encoder.set_bytes(4, std::mem::size_of::<u32>() as u64, &m as *const u32 as *const _);
        encoder.set_bytes(5, std::mem::size_of::<u32>() as u64, &k as *const u32 as *const _);
        encoder.set_bytes(6, std::mem::size_of::<u32>() as u64, &n as *const u32 as *const _);
        let activation_val = activation as u32;
        encoder.set_bytes(7, std::mem::size_of::<u32>() as u64, &activation_val as *const u32 as *const _);
        let has_bias = false;
        encoder.set_bytes(8, std::mem::size_of::<bool>() as u64, &has_bias as *const bool as *const _);

        // Use thread groups (tiled) or threads (naive) based on kernel
        if kernel_name.contains("tiled") {
            let threadgroup_size = metal::MTLSize::new(tile_size, tile_size, 1);
            let threadgroups = metal::MTLSize::new(
                (n as u64 + tile_size - 1) / tile_size,
                (m as u64 + tile_size - 1) / tile_size,
                1,
            );
            encoder.dispatch_thread_groups(threadgroups, threadgroup_size);
        } else {
            let grid_size = metal::MTLSize::new(n as u64, m as u64, 1);
            let threadgroup_size = metal::MTLSize::new(16.min(n as u64), 16.min(m as u64), 1);
            encoder.dispatch_threads(grid_size, threadgroup_size);
        }

        encoder.end_encoding();
        command_buffer.commit();
        crate::ops::async_exec::submit_async(&command_buffer);

        Tensor::new(
            BufferHandle::Metal(unsafe { std::mem::transmute(result_buf) }),
            crate::tensor::TensorShape::new(output_shape),
            self.device().clone(),
        )
    }

    /// Fused affine transformation: self * scale + bias
    ///
    /// Used in batch normalization and similar operations.
    pub fn fused_affine(&self, scale: &Tensor, bias: &Tensor<T>) -> TensorResult<Self> {
        if !self.shape().is_same(scale.shape()) || !self.shape().is_same(bias.shape()) {
            return Err(TensorError::ShapeMismatch {
                expected: self.dims().to_vec(),
                actual: scale.dims().to_vec(),
            });
        }

        match self.device() {
            Device::Metal(_) => self.fused_affine_metal(scale, bias),
            Device::CPU => self.fused_affine_cpu(scale, bias),
            Device::NeuralEngine => self.fused_affine_neural_engine(scale, bias),
        }
    }

    /// Metal GPU implementation of fused affine
    fn fused_affine_metal(&self, scale: &Tensor, bias: &Tensor<T>) -> TensorResult<Self> {
        // Currently only f16 is supported for Metal operations
        if false {
            return Err(TensorError::InvalidOperation(
                "Metal operations currently only support f16".to_string()
            ));
        }

        let x_buf = self.buffer().as_metal()?;
        let scale_buf = scale.buffer().as_metal()?;
        let bias_buf = bias.buffer().as_metal()?;

        let mut device = match self.device() {
            Device::Metal(dev) => dev.clone(),
            _ => return Err(TensorError::DeviceConversionError("Not on Metal device".to_string())),
        };

        if device.library().is_none() {
            let shader_source = include_str!("../../shaders/unified.metal");
            device.load_library(shader_source)?;
        }

        let result_buf = MetalBuffer::<T>::new_uninit_pooled(device.buffer_pool(), self.numel())?;

        let mut executor = crate::device::KernelExecutor::new(device.clone());
        let pipeline = executor.get_or_compile_pipeline("fused_affine_f16")?;

        let (_flushed, command_buffer) = device.command_buffer()?;
        let encoder = command_buffer.as_ref().new_compute_command_encoder();

        encoder.set_compute_pipeline_state(&pipeline);
        encoder.set_buffer(0, Some(&x_buf.buffer), 0);
        encoder.set_buffer(1, Some(&scale_buf.buffer), 0);
        encoder.set_buffer(2, Some(&bias_buf.buffer), 0);
        encoder.set_buffer(3, Some(&result_buf.buffer), 0);

        let grid_size = metal::MTLSize::new(self.numel() as u64, 1, 1);
        let threadgroup_size = metal::MTLSize::new(256.min(self.numel() as u64), 1, 1);

        encoder.dispatch_threads(grid_size, threadgroup_size);
        encoder.end_encoding();
        command_buffer.commit();
        crate::ops::async_exec::submit_async(&command_buffer);

        Tensor::new(
            BufferHandle::Metal(unsafe { std::mem::transmute(result_buf) }),
            self.shape().clone(),
            self.device().clone(),
        )
    }

    /// CPU implementation of fused affine
    fn fused_affine_cpu(&self, scale: &Tensor, bias: &Tensor<T>) -> TensorResult<Self> {
        panic!("src/ops/fused.rs:423:5");
        // Currently only f16 is supported
        if false {
            return Err(TensorError::InvalidOperation(
                "CPU operations currently only support f16".to_string()
            ));
        }

        let x_data = self.buffer().to_cpu_vec();
        let scale_data = scale.buffer().to_cpu_vec();
        let bias_data = bias.buffer().to_cpu_vec();

        // Safety: We checked T::is_f16() in the caller
        let x_f16: Vec<f16> = unsafe { std::mem::transmute(x_data) };
        let scale_f16: Vec<f16> = unsafe { std::mem::transmute(scale_data) };
        let bias_f16: Vec<f16> = unsafe { std::mem::transmute(bias_data) };

        let result: Vec<f16> = x_f16
            .iter()
            .zip(scale_f16.iter())
            .zip(bias_f16.iter())
            .map(|((&x, &s), &b)| x * s + b)
            .collect();
        let result_t: Vec<T> = unsafe { std::mem::transmute(result) };

        Tensor::from_vec(result_t, self.dims().to_vec())
    }

    /// Neural Engine implementation of fused add + relu
    fn fused_add_relu_neural_engine(&self, other: &Tensor<T>) -> TensorResult<Self> {
        let a_buf = self.buffer().as_neural_engine()?;
        let b_buf = other.buffer().as_neural_engine()?;

        let result_buf = NeuralEngineOps::fused_add_relu(a_buf, b_buf)?;

        Tensor::new(
            BufferHandle::NeuralEngine(result_buf),
            self.shape().clone(),
            self.device().clone(),
        )
    }

    /// Neural Engine implementation of fused mul + relu
    fn fused_mul_relu_neural_engine(&self, other: &Tensor<T>) -> TensorResult<Self> {
        let a_buf = self.buffer().as_neural_engine()?;
        let b_buf = other.buffer().as_neural_engine()?;

        let result_buf = NeuralEngineOps::fused_mul_relu(a_buf, b_buf)?;

        Tensor::new(
            BufferHandle::NeuralEngine(result_buf),
            self.shape().clone(),
            self.device().clone(),
        )
    }

    /// Neural Engine implementation of fused affine
    fn fused_affine_neural_engine(&self, scale: &Tensor, bias: &Tensor<T>) -> TensorResult<Self> {
        let x_buf = self.buffer().as_neural_engine()?;
        let scale_buf = scale.buffer().as_neural_engine()?;
        let bias_buf = bias.buffer().as_neural_engine()?;

        let result_buf = NeuralEngineOps::fused_affine(x_buf, scale_buf, bias_buf)?;

        Tensor::new(
            BufferHandle::NeuralEngine(result_buf),
            self.shape().clone(),
            self.device().clone(),
        )
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::device::MetalDevice;

    fn get_test_device() -> MetalDevice {
        MetalDevice::new().expect("No Metal device available")
    }

    #[test]
    fn test_fused_add_relu() {
        let device = get_test_device();

        let a = Tensor::from_vec_gpu(
            &device,
            vec![f16::from_f32(1.0), f16::from_f32(-2.0), f16::from_f32(3.0), f16::from_f32(-4.0)],
            vec![4],
        )
        .unwrap();

        let b = Tensor::from_vec_gpu(
            &device,
            vec![f16::from_f32(-1.0), f16::from_f32(3.0), f16::from_f32(-2.0), f16::from_f32(5.0)],
            vec![4],
        )
        .unwrap();

        let result = a.fused_add_relu(&b).unwrap();
        let result_data = result.to_vec();

        // Expected: max(1-1, 0) = 0, max(-2+3, 0) = 1, max(3-2, 0) = 1, max(-4+5, 0) = 1
        assert_eq!(result_data[0], f16::from_f32(0.0));
        assert_eq!(result_data[1], f16::from_f32(1.0));
        assert_eq!(result_data[2], f16::from_f32(1.0));
        assert_eq!(result_data[3], f16::from_f32(1.0));
    }

    #[test]
    fn test_fused_mul_relu() {
        let device = get_test_device();

        let a = Tensor::from_vec_gpu(
            &device,
            vec![f16::from_f32(2.0), f16::from_f32(-3.0), f16::from_f32(4.0)],
            vec![3],
        )
        .unwrap();

        let b = Tensor::from_vec_gpu(
            &device,
            vec![f16::from_f32(0.5), f16::from_f32(2.0), f16::from_f32(-1.0)],
            vec![3],
        )
        .unwrap();

        let result = a.fused_mul_relu(&b).unwrap();
        let result_data = result.to_vec();

        // Expected: max(2*0.5, 0) = 1, max(-3*2, 0) = 0, max(4*-1, 0) = 0
        assert_eq!(result_data[0], f16::from_f32(1.0));
        assert_eq!(result_data[1], f16::from_f32(0.0));
        assert_eq!(result_data[2], f16::from_f32(0.0));
    }

    #[test]
    fn test_fused_affine() {
        let device = get_test_device();

        let x = Tensor::from_vec_gpu(
            &device,
            vec![f16::from_f32(1.0), f16::from_f32(2.0), f16::from_f32(3.0)],
            vec![3],
        )
        .unwrap();

        let scale = Tensor::from_vec_gpu(
            &device,
            vec![f16::from_f32(2.0), f16::from_f32(3.0), f16::from_f32(4.0)],
            vec![3],
        )
        .unwrap();

        let bias = Tensor::from_vec_gpu(
            &device,
            vec![f16::from_f32(1.0), f16::from_f32(2.0), f16::from_f32(3.0)],
            vec![3],
        )
        .unwrap();

        let result = x.fused_affine(&scale, &bias).unwrap();
        let result_data = result.to_vec();

        // Expected: 1*2+1=3, 2*3+2=8, 3*4+3=15
        assert_eq!(result_data[0], f16::from_f32(3.0));
        assert_eq!(result_data[1], f16::from_f32(8.0));
        assert_eq!(result_data[2], f16::from_f32(15.0));
    }

    #[test]
    fn test_fused_vs_unfused() {
        let device = get_test_device();

        let a = Tensor::from_vec_gpu(
            &device,
            vec![f16::from_f32(1.0), f16::from_f32(-2.0), f16::from_f32(3.0)],
            vec![3],
        )
        .unwrap();

        let b = Tensor::from_vec_gpu(
            &device,
            vec![f16::from_f32(2.0), f16::from_f32(3.0), f16::from_f32(-1.0)],
            vec![3],
        )
        .unwrap();

        // Fused version
        let fused_result = a.fused_add_relu(&b).unwrap();

        // Unfused version
        let unfused_result = a.add(&b).unwrap().relu().unwrap();

        // Results should be identical
        assert_eq!(fused_result.to_vec(), unfused_result.to_vec());
    }

    #[test]
    fn test_matmul_relu_fusion() {
        let device = get_test_device();

        let a = Tensor::from_vec_gpu(&device, vec![f16::ONE; 4], vec![2, 2]).unwrap();
        let b = Tensor::from_vec_gpu(&device, vec![f16::ONE; 4], vec![2, 2]).unwrap();

        let fused = a.matmul_with_activation(&b, Activation::ReLU).unwrap();
        let unfused = a.matmul(&b).unwrap().relu().unwrap();

        // Should be equivalent
        let fused_data = fused.to_vec();
        let unfused_data = unfused.to_vec();

        for (f, u) in fused_data.iter().zip(unfused_data.iter()) {
            assert!((f.to_f32() - u.to_f32()).abs() < 0.01);
        }
    }
}

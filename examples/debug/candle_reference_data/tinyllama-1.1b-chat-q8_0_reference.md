================================================================================
CANDLE REFERENCE VALUE EXTRACTOR
================================================================================

Model: /Users/junsuzuki/.llm/models/tinyllama-1.1b-chat-q8_0.gguf
Device: CPU (for reference extraction)

Loading model with Candle...
Model loaded successfully!
Number of tensors: 201

## Extracting Key Tensors ##


Processing: token_embd.weight
Original GGUF shape: [32000, 2048]
GGUF dtype: Q8_0

================================================================================
Tensor: token_embd.weight
================================================================================
Shape: [32000, 2048]
DType: F32

First 10 values:
[0]: 0.0000061989
[1]: 0.0000042915
[2]: 0.0000041723
[3]: 0.0000095367
[4]: 0.0000046492
[5]: -0.0000052452
[6]: -0.0000060797
[7]: 0.0000096560
[8]: 0.0000116825
[9]: 0.0000056028

Statistics:
  Sum:  -24.2523326874
  Mean: -0.0000003701
  Min:  -0.1171803474
  Max:  0.1494579315

**BOS Token (ID=1) Embedding:**
  Sum: 0.0528666377
  Mean: 0.0000258138

Processing: blk.0.attn_norm.weight
Original GGUF shape: [2048]
GGUF dtype: F32

================================================================================
Tensor: blk.0.attn_norm.weight
================================================================================
Shape: [2048]
DType: F32

First 10 values:
[0]: -0.0041809082
[1]: 0.0063171387
[2]: 0.0698242188
[3]: -0.0294189453
[4]: -0.0061340332
[5]: -0.0151977539
[6]: 0.0045776367
[7]: 0.0041503906
[8]: 0.0036315918
[9]: 0.0072021484

Statistics:
  Sum:  11.8375186920
  Mean: 0.0057800384
  Min:  -0.5820312500
  Max:  0.7695312500

Processing: blk.0.attn_q.weight
Original GGUF shape: [2048, 2048]
GGUF dtype: Q8_0

================================================================================
Tensor: blk.0.attn_q.weight
================================================================================
Shape: [2048, 2048]
DType: F32

First 10 values:
[0]: -0.0014365911
[1]: -0.0024311543
[2]: -0.0074039698
[3]: -0.0140343904
[4]: -0.0028731823
[5]: -0.0027626753
[6]: 0.0008840561
[7]: -0.0029836893
[8]: 0.0012155771
[9]: 0.0138133764

Statistics:
  Sum:  -31.2397632599
  Mean: -0.0000074481
  Min:  -1.5861434937
  Max:  1.0309448242

Processing: blk.0.attn_k.weight
Original GGUF shape: [256, 2048]
GGUF dtype: Q8_0

================================================================================
Tensor: blk.0.attn_k.weight
================================================================================
Shape: [256, 2048]
DType: F32

First 10 values:
[0]: -0.0030212402
[1]: 0.0030212402
[2]: -0.0196380615
[3]: 0.0319747925
[4]: -0.0050354004
[5]: 0.0100708008
[6]: 0.0000000000
[7]: -0.0050354004
[8]: 0.0015106201
[9]: -0.0151062012

Statistics:
  Sum:  -52.1654663086
  Mean: -0.0000994977
  Min:  -3.1102752686
  Max:  1.1801605225

Processing: blk.0.attn_v.weight
Original GGUF shape: [256, 2048]
GGUF dtype: Q8_0

================================================================================
Tensor: blk.0.attn_v.weight
================================================================================
Shape: [256, 2048]
DType: F32

First 10 values:
[0]: 0.0279600620
[1]: 0.0060532093
[2]: -0.0002882481
[3]: -0.0054767132
[4]: 0.0074944496
[5]: -0.0077826977
[6]: 0.0066297054
[7]: -0.0158536434
[8]: 0.0366075039
[9]: -0.0017294884

Statistics:
  Sum:  5.9289512634
  Mean: 0.0000113086
  Min:  -0.0674014091
  Max:  0.0620117188

Processing: output_norm.weight
Original GGUF shape: [2048]
GGUF dtype: F32

================================================================================
Tensor: output_norm.weight
================================================================================
Shape: [2048]
DType: F32

First 10 values:
[0]: 1.9218750000
[1]: 1.8203125000
[2]: 1.9453125000
[3]: 1.9843750000
[4]: 1.9140625000
[5]: 1.9062500000
[6]: 1.9140625000
[7]: 1.6640625000
[8]: 1.9296875000
[9]: 1.9765625000

Statistics:
  Sum:  3921.6875000000
  Mean: 1.9148864746
  Min:  0.5078125000
  Max:  3.1875000000

================================================================================
âœ… Reference extraction complete
================================================================================

================================================================================
CANDLE REFERENCE VALUE EXTRACTOR
================================================================================

Model: /Users/junsuzuki/.llm/models/tinyllama-1.1b-chat-q5_0.gguf
Device: CPU (for reference extraction)

Loading model with Candle...
Model loaded successfully!
Number of tensors: 201

## Extracting Key Tensors ##


Processing: token_embd.weight
Original GGUF shape: [32000, 2048]
GGUF dtype: Q5_0

================================================================================
Tensor: token_embd.weight
================================================================================
Shape: [32000, 2048]
DType: F32

First 10 values:
[0]: 0.0000066757
[1]: 0.0000047684
[2]: 0.0000038147
[3]: 0.0000095367
[4]: 0.0000047684
[5]: -0.0000057220
[6]: -0.0000057220
[7]: 0.0000095367
[8]: 0.0000114441
[9]: 0.0000057220

Statistics:
  Sum:  -24.7019577026
  Mean: -0.0000003769
  Min:  -0.1171875000
  Max:  0.1494140625

**BOS Token (ID=1) Embedding:**
  Sum: 0.0503845215
  Mean: 0.0000246018

Processing: blk.0.attn_norm.weight
Original GGUF shape: [2048]
GGUF dtype: F32

================================================================================
Tensor: blk.0.attn_norm.weight
================================================================================
Shape: [2048]
DType: F32

First 10 values:
[0]: -0.0041809082
[1]: 0.0063171387
[2]: 0.0698242188
[3]: -0.0294189453
[4]: -0.0061340332
[5]: -0.0151977539
[6]: 0.0045776367
[7]: 0.0041503906
[8]: 0.0036315918
[9]: 0.0072021484

Statistics:
  Sum:  11.8375186920
  Mean: 0.0057800384
  Min:  -0.5820312500
  Max:  0.7695312500

Processing: blk.0.attn_q.weight
Original GGUF shape: [2048, 2048]
GGUF dtype: Q5_0

================================================================================
Tensor: blk.0.attn_q.weight
================================================================================
Shape: [2048, 2048]
DType: F32

First 10 values:
[0]: -0.0017547607
[1]: -0.0026321411
[2]: -0.0070190430
[3]: -0.0140380859
[4]: -0.0026321411
[5]: -0.0026321411
[6]: 0.0008773804
[7]: -0.0026321411
[8]: 0.0008773804
[9]: 0.0131607056

Statistics:
  Sum:  -28.4793357849
  Mean: -0.0000067900
  Min:  -1.5859375000
  Max:  1.0312500000

Processing: blk.0.attn_k.weight
Original GGUF shape: [256, 2048]
GGUF dtype: Q5_0

================================================================================
Tensor: blk.0.attn_k.weight
================================================================================
Shape: [256, 2048]
DType: F32

First 10 values:
[0]: -0.0039978027
[1]: 0.0039978027
[2]: -0.0199890137
[3]: 0.0319824219
[4]: -0.0059967041
[5]: 0.0099945068
[6]: -0.0000000000
[7]: -0.0059967041
[8]: 0.0019989014
[9]: -0.0159912109

Statistics:
  Sum:  -53.6377143860
  Mean: -0.0001023058
  Min:  -3.1093750000
  Max:  1.1796875000

Processing: blk.0.attn_v.weight
Original GGUF shape: [256, 2048]
GGUF dtype: Q5_0

================================================================================
Tensor: blk.0.attn_v.weight
================================================================================
Shape: [256, 2048]
DType: F32

First 10 values:
[0]: 0.0274658203
[1]: 0.0068664551
[2]: -0.0000000000
[3]: -0.0045776367
[4]: 0.0068664551
[5]: -0.0068664551
[6]: 0.0068664551
[7]: -0.0160217285
[8]: 0.0366210938
[9]: -0.0022888184

Statistics:
  Sum:  6.1120738983
  Mean: 0.0000116579
  Min:  -0.0673828125
  Max:  0.0620117188

Processing: output_norm.weight
Original GGUF shape: [2048]
GGUF dtype: F32

================================================================================
Tensor: output_norm.weight
================================================================================
Shape: [2048]
DType: F32

First 10 values:
[0]: 1.9218750000
[1]: 1.8203125000
[2]: 1.9453125000
[3]: 1.9843750000
[4]: 1.9140625000
[5]: 1.9062500000
[6]: 1.9140625000
[7]: 1.6640625000
[8]: 1.9296875000
[9]: 1.9765625000

Statistics:
  Sum:  3921.6875000000
  Mean: 1.9148864746
  Min:  0.5078125000
  Max:  3.1875000000

================================================================================
âœ… Reference extraction complete
================================================================================

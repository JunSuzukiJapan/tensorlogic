main {
    print("=== Autoregressive Text Generation Example ===")
    print("")
    print("This example demonstrates the complete autoregressive")
    print("generation loop used in modern LLMs like GPT, LLaMA, etc.")
    print("")

    let vocab_size = 1000
    let max_new_tokens = 10
    let temperature = 0.7
    let top_k = 50
    let top_p = 0.9

    print("Configuration:")
    print("  Vocabulary size:", vocab_size)
    print("  Max new tokens:", max_new_tokens)
    print("  Temperature:", temperature)
    print("  Top-k:", top_k)
    print("  Top-p:", top_p)
    print("")

    print("=== Generation Loop ===")
    print("")
    print("Simulating autoregressive token generation...")
    print("(In production, this would use actual Transformer forward pass)")
    print("")

    print("Initial prompt: \"The capital of Japan is\"")
    print("Generating", max_new_tokens, "tokens...")
    print("")

    print("Token generation steps (demonstrating 3 iterations):")
    print("")

    print("  [Step 0]")
    print("    1. Forward pass: input_ids → logits")
    let logits_2d_0 = positional_encoding(1, vocab_size)
    let logits_0 = flatten(logits_2d_0)
    print("       ✓ Logits shape: [", vocab_size, "]")

    print("    2. Apply top-k filter (k =", top_k, ")")
    let filtered_k_0 = top_k(logits_0, top_k)
    print("       ✓ Filtered to top", top_k, "tokens")

    print("    3. Apply top-p filter (p =", top_p, ")")
    let filtered_0 = top_p(filtered_k_0, top_p)
    print("       ✓ Applied nucleus sampling")

    print("    4. Convert to probabilities")
    let probs_0 = softmax(filtered_0, 0)
    print("       ✓ Probability distribution computed")

    print("    5. Sample next token")
    let token_0 = sample(probs_0)
    print("       ✓ Sampled token ID:", token_0)
    print("    6. Append to sequence")
    print("       ✓ input_ids = append(input_ids, ", token_0, ")")
    print("")

    print("  [Step 1]")
    print("    1. Forward pass: input_ids → logits")
    let logits_2d_1 = positional_encoding(1, vocab_size)
    let logits_1 = flatten(logits_2d_1)
    print("       ✓ Logits shape: [", vocab_size, "]")
    print("    2-5. Apply sampling pipeline")
    let filtered_k_1 = top_k(logits_1, top_k)
    let filtered_1 = top_p(filtered_k_1, top_p)
    let probs_1 = softmax(filtered_1, 0)
    let token_1 = sample(probs_1)
    print("       ✓ Sampled token ID:", token_1)
    print("    6. Append to sequence")
    print("")

    print("  [Step 2]")
    print("    1. Forward pass: input_ids → logits")
    let logits_2d_2 = positional_encoding(1, vocab_size)
    let logits_2 = flatten(logits_2d_2)
    print("       ✓ Logits shape: [", vocab_size, "]")
    print("    2-5. Apply sampling pipeline")
    let filtered_k_2 = top_k(logits_2, top_k)
    let filtered_2 = top_p(filtered_k_2, top_p)
    let probs_2 = softmax(filtered_2, 0)
    let token_2 = sample(probs_2)
    print("       ✓ Sampled token ID:", token_2)
    print("    6. Append to sequence")
    print("")

    print("  ... (continue until max_tokens or EOS)")

    print("")
    print("=== Generation Complete! ===")
    print("")
    print("Demonstrated 3 token generation steps")
    print("")

    print("=== Production Implementation Details ===")
    print("")
    print("Key Components:")
    print("")
    print("1. KV-Cache Optimization")
    print("   • Cache Key and Value tensors from previous tokens")
    print("   • Only compute attention for new token position")
    print("   • Drastically reduces computation for long sequences")
    print("   • Example: 100 tokens → 100x speedup vs recomputing all")
    print("")
    print("2. Temperature Scaling")
    print("   • logits = logits / temperature")
    print("   • temperature < 1.0 → more confident/deterministic")
    print("   • temperature > 1.0 → more random/creative")
    print("   • temperature → 0   → greedy decoding (argmax)")
    print("")
    print("3. Sampling Strategies")
    print("   • Greedy: Always pick highest probability token")
    print("   • Top-k: Sample from k most likely tokens")
    print("   • Top-p: Sample from smallest set with cumulative p")
    print("   • Combined: top-k → top-p → sample (recommended)")
    print("")
    print("4. Special Tokens")
    print("   • BOS (Beginning of Sequence): Start of text")
    print("   • EOS (End of Sequence): Stop generation")
    print("   • PAD: Padding for batch processing")
    print("")
    print("5. Batched Generation")
    print("   • Generate multiple sequences in parallel")
    print("   • Use padding to handle different lengths")
    print("   • Beam search for higher quality outputs")
    print("")

    print("=== Example Output (Conceptual) ===")
    print("")
    print("Prompt:    The capital of Japan is")
    print("Generated: Tokyo, a bustling metropolis known for")
    print("")
    print("Token-by-token breakdown:")
    print("  Step 0: \"The capital of Japan is\" → sample → \"Tokyo\"")
    print("  Step 1: \"... is Tokyo\" → sample → \",\"")
    print("  Step 2: \"... Tokyo,\" → sample → \"a\"")
    print("  Step 3: \"... , a\" → sample → \"bustling\"")
    print("  Step 4: \"... a bustling\" → sample → \"metropolis\"")
    print("  Step 5: \"... bustling metropolis\" → sample → \"known\"")
    print("  Step 6: \"... metropolis known\" → sample → \"for\"")
    print("  Step 7: \"... known for\" → sample → EOS or continue...")
    print("")

    print("=== Real-World Hyperparameters ===")
    print("")
    print("Chat Models (GPT-4, Claude, LLaMA):")
    print("  • Temperature: 0.7-1.0 (balanced)")
    print("  • Top-p: 0.9 (nucleus sampling)")
    print("  • Top-k: 50-100 (quality control)")
    print("  • Max tokens: 2048-4096")
    print("")
    print("Code Generation:")
    print("  • Temperature: 0.1-0.3 (more deterministic)")
    print("  • Top-p: 0.95")
    print("  • Top-k: 40")
    print("  • Max tokens: 512-2048")
    print("")
    print("Creative Writing:")
    print("  • Temperature: 1.0-1.2 (more random)")
    print("  • Top-p: 0.85")
    print("  • Top-k: 100")
    print("  • Max tokens: 2048-8192")
    print("")

    print("=== Next Steps for Implementation ===")
    print("")
    print("1. Implement Transformer forward pass in Rust")
    print("   • Load actual model weights from GGUF file")
    print("   • Implement multi-layer attention mechanism")
    print("   • Integrate with Metal GPU acceleration")
    print("")
    print("2. Add KV-Cache for efficient generation")
    print("   • Cache mechanism in interpreter")
    print("   • Memory management for long contexts")
    print("")
    print("3. Implement temperature scaling")
    print("   • Add temperature parameter to generate()")
    print("   • Scale logits before sampling")
    print("")
    print("4. Create interactive REPL")
    print("   • Multi-turn conversation support")
    print("   • Chat history management")
    print("   • System prompt configuration")
    print("")

    print("✅ Autoregressive generation concept complete!")
}

# Building Language Models with TensorLogic

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€TensorLogicã‚’ä½¿ã£ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã®åŸºæœ¬æ¦‚å¿µã‚’å­¦ã¶æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## LLMã®åŸºæœ¬ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

### 1. Token Embeddingsï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ï¼‰

**ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**: é›¢æ•£çš„ãªãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå˜èªã‚„æ–‡å­—ï¼‰ã‚’é€£ç¶šçš„ãªãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›

**TensorLogicã§ã®å®Ÿè£…ä¾‹**:
```tensorlogic
// èªå½™ã‚µã‚¤ã‚º8ã€åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ16
tensor emb_0: float16[16] learnable = randn([16]) * 0.1
tensor emb_1: float16[16] learnable = randn([16]) * 0.1
// ... ä»–ã®ãƒˆãƒ¼ã‚¯ãƒ³ç”¨ã®åŸ‹ã‚è¾¼ã¿
```

**å‚è€ƒexample**: `examples/gnn_node_classification.tl` ã®ãƒãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿éƒ¨åˆ†

### 2. Feedforward Neural Networkï¼ˆé †ä¼æ’­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰

**ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**: å…¥åŠ›ã‚’å¤‰æ›ã—ã¦é«˜ãƒ¬ãƒ™ãƒ«ã®è¡¨ç¾ã‚’å­¦ç¿’

**TensorLogicã§ã®å®Ÿè£…ä¾‹**:
```tensorlogic
// éš ã‚Œå±¤
tensor W1: float16[16, 32] learnable = randn([16, 32]) * 0.1
tensor b1: float16[32] learnable = zeros([32])

// Forward pass
tensor hidden: float16[32] = relu(matmul(input, W1) + b1)
```

**å‚è€ƒexample**: `examples/gnn_node_classification.tl` ã®GNNå±¤

### 3. Attention Mechanismï¼ˆæ³¨æ„æ©Ÿæ§‹ï¼‰

**ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç•°ãªã‚‹éƒ¨åˆ†ã«æ³¨ç›®ã™ã‚‹èƒ½åŠ›ï¼ˆTransformerã®æ ¸å¿ƒï¼‰

**æ•°å¼**:
```
Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V
```

**TensorLogicã§ã®å®Ÿè£…**:
```tensorlogic
// Query, Key, Value projections
tensor Q: float16[seq_len, d_k] = matmul(input, W_q)
tensor K: float16[seq_len, d_k] = matmul(input, W_k)
tensor V: float16[seq_len, d_v] = matmul(input, W_v)

// Attention scores
tensor scores: float16[seq_len, seq_len] = matmul(Q, transpose(K))
tensor attn_weights: float16[seq_len, seq_len] = softmax(scores / sqrt_dk)

// Attention output
tensor output: float16[seq_len, d_v] = matmul(attn_weights, V)
```

**å‚è€ƒexample**: `examples/attention.tl` - å®Œå…¨ãªattentionå®Ÿè£…ã‚’å‚ç…§

### 4. Next Token Predictionï¼ˆæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰

**ã‚³ãƒ³ã‚»ãƒ—ãƒˆ**: LLMã®åŸºæœ¬ã‚¿ã‚¹ã‚¯ - ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬

**TensorLogicã§ã®å®Ÿè£…**:
```tensorlogic
learn {
    // Input embedding
    tensor input_emb: float16[embedding_dim]
    input_emb = embeddings[token_id]

    // Forward pass
    tensor hidden: float16[hidden_dim] = relu(matmul(input_emb, W1) + b1)
    tensor logits: float16[vocab_size] = matmul(hidden, W_out) + b_out

    // Softmax for probability distribution
    tensor probs: float16[vocab_size] = softmax(logits)

    // Target (one-hot vector for next token)
    tensor target: float16[vocab_size] = [0.0, ..., 1.0, ..., 0.0]

    // Training objective
    minimize mean((probs - target) ** 2.0)
    with adam(lr=0.01)
    epochs 100
}
```

## å®Ÿè·µã‚¬ã‚¤ãƒ‰ï¼šLLMé¢¨ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰æ‰‹é †

### ã‚¹ãƒ†ãƒƒãƒ—1: èªå½™ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©

```tensorlogic
main {
    // èªå½™: 10ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆä¾‹: é »å‡ºå˜èªï¼‰
    // åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ: 16
    // éš ã‚Œå±¤æ¬¡å…ƒ: 32

    tensor emb_0: float16[16] learnable = randn([16]) * 0.1
    tensor emb_1: float16[16] learnable = randn([16]) * 0.1
    // ... emb_2 ã‹ã‚‰ emb_9 ã¾ã§

    tensor W_hidden: float16[16, 32] learnable = randn([16, 32]) * 0.1
    tensor b_hidden: float16[32] learnable = zeros([32])

    tensor W_output: float16[32, 10] learnable = randn([32, 10]) * 0.1
    tensor b_output: float16[10] learnable = zeros([10])
}
```

### ã‚¹ãƒ†ãƒƒãƒ—2: å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…

```tensorlogic
learn {
    // ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒˆãƒ¼ã‚¯ãƒ³0 â†’ ãƒˆãƒ¼ã‚¯ãƒ³1
    tensor h1: float16[32] = relu(matmul(emb_0, W_hidden) + b_hidden)
    tensor logits1: float16[10] = matmul(h1, W_output) + b_output
    tensor probs1: float16[10] = softmax(logits1)

    tensor target1: float16[10] = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

    minimize mean((probs1 - target1) ** 2.0)
    with adam(lr=0.01)
    epochs 50
    report_every 10
}
```

### ã‚¹ãƒ†ãƒƒãƒ—3: æ¨è«–ã®å®Ÿè£…

```tensorlogic
// ãƒ†ã‚¹ãƒˆ: ãƒˆãƒ¼ã‚¯ãƒ³0ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬
tensor test_h: float16[32] = relu(matmul(emb_0, W_hidden) + b_hidden)
tensor test_logits: float16[10] = matmul(test_h, W_output) + b_output
tensor test_probs: float16[10] = softmax(test_logits)

print("Predicted probabilities:")
print("  Token 0:", [test_probs[0]])
print("  Token 1:", [test_probs[1]], "<-- Expected high")
// ...
```

## ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: ãƒŸãƒ‹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æœ¬ç‰©ã®LLMã¸

| è¦ç´  | TensorLogicãƒŸãƒ‹å®Ÿè£… | GPT-3/4ã‚¯ãƒ©ã‚¹ |
|------|-------------------|--------------|
| **èªå½™ã‚µã‚¤ã‚º** | 10-100ãƒˆãƒ¼ã‚¯ãƒ³ | 50,000-100,000ãƒˆãƒ¼ã‚¯ãƒ³ |
| **åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ** | 8-32 | 4,096-12,288 |
| **éš ã‚Œå±¤æ•°** | 1-2å±¤ | 96-120+å±¤ |
| **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°** | ~500-5000 | 1750å„„-1å…†7600å„„ |
| **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·** | 1-10ãƒˆãƒ¼ã‚¯ãƒ³ | 4,096-128,000ãƒˆãƒ¼ã‚¯ãƒ³ |
| **å­¦ç¿’æ™‚é–“** | ç§’-åˆ† | é€±-æœˆ |
| **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿** | æ‰‹ä½œã‚Šãƒ‘ã‚¿ãƒ¼ãƒ³ | ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆè¦æ¨¡ã®ãƒ†ã‚­ã‚¹ãƒˆ |

## ä¸»è¦ãªæ¦‚å¿µã®å‚è€ƒExample

### Attention Mechanism
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `examples/attention.tl`
- **å­¦ã¹ã‚‹ã“ã¨**:
  - Scaled Dot-Product Attention
  - Query, Key, Value projections
  - Attention weights ã®è¨ˆç®—

### Graph Neural Network (æ§‹é€ ãŒä¼¼ã¦ã„ã‚‹)
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `examples/gnn_node_classification.tl`
- **å­¦ã¹ã‚‹ã“ã¨**:
  - Embedding ã®å­¦ç¿’
  - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼ˆAttentionã®ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼READ)`
  - åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å®Ÿè£…

### Batch Normalization
- **ãƒ•ã‚¡ã‚¤ãƒ«**: `examples/batch_norm_demo.tl`
- **å­¦ã¹ã‚‹ã“ã¨**:
  - æ­£è¦åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯
  - å­¦ç¿’ã®å®‰å®šåŒ–

## å®Ÿè£…ä¸Šã®æ³¨æ„ç‚¹

### 1. TensorLogicã®åˆ¶ç´„

- **å›ºå®šã‚µã‚¤ã‚º**: ã™ã¹ã¦ã®ãƒ†ãƒ³ã‚½ãƒ«ã¯å›ºå®šã‚µã‚¤ã‚º
- **é™çš„ã‚°ãƒ©ãƒ•**: å‹•çš„ãªè¨ˆç®—ã‚°ãƒ©ãƒ•ã¯æœªã‚µãƒãƒ¼ãƒˆ
- **ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å‡¦ç†**: RNN/LSTMã¯ç¾çŠ¶ã§ã¯æ‰‹å‹•å®Ÿè£…ãŒå¿…è¦

### 2. æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

**åˆå¿ƒè€…å‘ã‘**:
1. `examples/attention.tl` ã‚’å®Ÿè¡Œã—ã¦ç†è§£
2. åŸ‹ã‚è¾¼ã¿ã®æ¦‚å¿µã‚’ `gnn_node_classification.tl` ã§å­¦ç¿’
3. ç°¡å˜ãªãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰

**ä¸­ç´šè€…å‘ã‘**:
1. Multi-head attentionã®å®Ÿè£…
2. Position encodingsã®è¿½åŠ 
3. è¤‡æ•°å±¤ã®Transformerãƒ–ãƒ­ãƒƒã‚¯

**ä¸Šç´šè€…å‘ã‘**:
1. ãƒŸãƒ‹Transformerã®å®Œå…¨å®Ÿè£…
2. ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥
3. Fine-tuningã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

## å­¦ç¿’ãƒ‘ã‚¹

```
1. åŸºç¤
   â†“
   - Tensoræ“ä½œã®ç†è§£
   - è¡Œåˆ—ä¹—ç®—ã€ReLUã€Softmax

2. åŸ‹ã‚è¾¼ã¿
   â†“
   - Token â†’ Vectorå¤‰æ›
   - é¡ä¼¼æ€§ã®å­¦ç¿’

3. Attention
   â†“
   - Query/Key/Value ã®æ¦‚å¿µ
   - Attention weightsã®è¨ˆç®—

4. Transformer Block
   â†“
   - Self-attention + FFN
   - Residual connections
   - Layer normalization

5. Language Model
   â†“
   - Next token prediction
   - Autoregressive generation
   - Training objectives
```

## è¿½åŠ ãƒªã‚½ãƒ¼ã‚¹

### è«–æ–‡
- "Attention Is All You Need" (Vaswani et al., 2017)
- "Language Models are Unsupervised Multitask Learners" (GPT-2, Radford et al., 2019)
- "BERT: Pre-training of Deep Bidirectional Transformers" (Devlin et al., 2019)

### ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒªã‚½ãƒ¼ã‚¹
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) by Jay Alammar
- [The Illustrated GPT-2](http://jalammar.github.io/illustrated-gpt2/) by Jay Alammar
- [Hugging Face Course](https://huggingface.co/course/chapter1/1)

### æ›¸ç±
- "Speech and Language Processing" by Dan Jurafsky & James H. Martin
- "Deep Learning" by Ian Goodfellow, Yoshua Bengio, Aaron Courville

## ã‚ˆãã‚ã‚‹è³ªå•

**Q: TensorLogicã§æœ¬ç‰©ã®LLMã‚’å‹•ã‹ã›ã¾ã™ã‹ï¼Ÿ**
A: TensorLogicã¯æ•™è‚²ç›®çš„ã§ã‚ã‚Šã€GPT-4ã®ã‚ˆã†ãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œã«ã¯é©ã—ã¦ã„ã¾ã›ã‚“ã€‚ã—ã‹ã—ã€**åŒã˜åŸç†**ã‚’å°è¦æ¨¡ã§å­¦ã¶ã«ã¯æœ€é©ã§ã™ã€‚

**Q: ã©ã“ã‹ã‚‰å§‹ã‚ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ**
A: ã¾ãš `examples/attention.tl` ã‚’å®Ÿè¡Œã—ã¦ã€Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç†è§£ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

**Q: ãªãœCross-Entropyã§ã¯ãªã MSEã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã™ã‹ï¼Ÿ**
A: ç°¡æ½”ã•ã®ãŸã‚ã§ã™ã€‚å®Ÿéš›ã®LLMã¯Cross-Entropyã‚’ä½¿ç”¨ã—ã¾ã™ãŒã€MSEã§ã‚‚å­¦ç¿’ã®æ¦‚å¿µã¯ç†è§£ã§ãã¾ã™ã€‚

**Q: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã¯ã©ã®ã‚ˆã†ã«å–å¾—ã—ã¾ã™ã‹ï¼Ÿ**
A: ç¾åœ¨ã®TensorLogicã§ã¯ã€ç¢ºç‡åˆ†å¸ƒã‚’å‡ºåŠ›ã¨ã—ã¦å¾—ã¾ã™ã€‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆtemperatureãƒ™ãƒ¼ã‚¹ãªã©ï¼‰ã¯æ‰‹å‹•ã§å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## ã¾ã¨ã‚

TensorLogicã‚’ä½¿ã£ã¦ã€LLMã®æ ¸å¿ƒçš„ãªæ¦‚å¿µã‚’ç†è§£ã§ãã¾ã™ï¼š

âœ… **Token Embeddings** - å˜èªã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
âœ… **Neural Networks** - ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
âœ… **Attention** - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ³¨ç›®
âœ… **Next Token Prediction** - è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŸºæœ¬ã‚¿ã‚¹ã‚¯
âœ… **Gradient Descent** - ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–

ã“ã‚Œã‚‰ã¯ã€GPT-4ã®ã‚ˆã†ãªæœ€å…ˆç«¯ã®LLMã§ã‚‚ä½¿ã‚ã‚Œã¦ã„ã‚‹**å…¨ãåŒã˜åŸºç¤**ã§ã™ï¼

---

Happy Learning! ğŸš€

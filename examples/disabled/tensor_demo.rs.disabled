use tensorlogic::{MetalDevice, Tensor};
use tensorlogic::tensor::TensorShape;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("=== TensorLogic f16 Demo ===\n");

    // Initialize Metal device
    let device = MetalDevice::new()?;
    println!("Metal Device: {}", device.name());
    println!("Supports f16: {}\n", device.supports_f16());

    // Create tensors
    println!("Creating tensors...");
    let a = Tensor::ones(&device, vec![3])?;
    let b = Tensor::from_vec_gpu(
        &device,
        vec![
            half::f16::from_f32(2.0),
            half::f16::from_f32(3.0),
            half::f16::from_f32(4.0),
        ],
        vec![3],
    )?;

    println!("a = {:?}", a.to_vec_f32());
    println!("b = {:?}", b.to_vec_f32());

    // Perform operations
    println!("\nPerforming operations...");

    let c = a.add(&b)?;
    println!("a + b = {:?}", c.to_vec_f32());

    let d = b.mul(&b)?;
    println!("b * b = {:?}", d.to_vec_f32());

    // Test activation functions
    println!("\nTesting activation functions...");
    let x = Tensor::from_vec_gpu(
        &device,
        vec![
            half::f16::from_f32(-2.0),
            half::f16::from_f32(-1.0),
            half::f16::from_f32(0.0),
            half::f16::from_f32(1.0),
            half::f16::from_f32(2.0),
        ],
        vec![5],
    )?;

    let relu_result = x.relu()?;
    println!("ReLU({:?}) = {:?}", x.to_vec_f32(), relu_result.to_vec_f32());

    let gelu_result = x.gelu()?;
    println!("GELU({:?}) = {:?}", x.to_vec_f32(), gelu_result.to_vec_f32());

    let softmax_input = Tensor::from_vec_gpu(
        &device,
        vec![
            half::f16::from_f32(1.0),
            half::f16::from_f32(2.0),
            half::f16::from_f32(3.0),
        ],
        vec![3],
    )?;
    let softmax_result = softmax_input.softmax()?;
    println!("Softmax({:?}) = {:?}", softmax_input.to_vec_f32(), softmax_result.to_vec_f32());

    // Test matrix multiplication
    println!("\nTesting matrix multiplication...");
    let mat_a = Tensor::from_vec_gpu(
        &device,
        vec![
            half::f16::from_f32(1.0),
            half::f16::from_f32(2.0),
            half::f16::from_f32(3.0),
            half::f16::from_f32(4.0),
            half::f16::from_f32(5.0),
            half::f16::from_f32(6.0),
        ],
        vec![2, 3],
    )?;

    let mat_b = Tensor::from_vec_gpu(
        &device,
        vec![
            half::f16::from_f32(1.0),
            half::f16::from_f32(2.0),
            half::f16::from_f32(3.0),
            half::f16::from_f32(4.0),
            half::f16::from_f32(5.0),
            half::f16::from_f32(6.0),
        ],
        vec![3, 2],
    )?;

    let mat_c = mat_a.matmul(&mat_b)?;
    println!("Matrix A [2x3]: {:?}", mat_a.to_vec_f32());
    println!("Matrix B [3x2]: {:?}", mat_b.to_vec_f32());
    println!("A @ B [2x2]: {:?}", mat_c.to_vec_f32());

    // Test broadcasting
    println!("\nTesting broadcasting...");
    let vec = Tensor::from_vec_gpu(
        &device,
        vec![
            half::f16::from_f32(1.0),
            half::f16::from_f32(2.0),
            half::f16::from_f32(3.0),
        ],
        vec![3],
    )?;

    let broadcast_shape = TensorShape::new(vec![2, 3]);
    let broadcasted = vec.broadcast_to(&broadcast_shape)?;
    println!("Vector [3]: {:?}", vec.to_vec_f32());
    println!("Broadcasted to [2x3]: {:?}", broadcasted.to_vec_f32());

    // Test reduction operations
    println!("\nTesting reduction operations...");
    let matrix = Tensor::from_vec_gpu(
        &device,
        vec![
            half::f16::from_f32(1.0),
            half::f16::from_f32(2.0),
            half::f16::from_f32(3.0),
            half::f16::from_f32(4.0),
            half::f16::from_f32(5.0),
            half::f16::from_f32(6.0),
        ],
        vec![2, 3],
    )?;

    println!("Matrix [2x3]: {:?}", matrix.to_vec_f32());
    println!("Sum: {}", matrix.sum()?.to_f32());
    println!("Mean: {}", matrix.mean()?.to_f32());
    println!("Max: {}", matrix.max()?.to_f32());
    println!("Min: {}", matrix.min()?.to_f32());

    let sum_rows = matrix.sum_dim(0, false)?;
    println!("Sum along rows: {:?}", sum_rows.to_vec_f32());

    let sum_cols = matrix.sum_dim(1, false)?;
    println!("Sum along columns: {:?}", sum_cols.to_vec_f32());

    // Test einsum operations
    println!("\nTesting einsum operations...");

    // Matrix multiplication via einsum
    let a = Tensor::from_vec_gpu(
        &device,
        vec![
            half::f16::from_f32(1.0),
            half::f16::from_f32(2.0),
            half::f16::from_f32(3.0),
            half::f16::from_f32(4.0),
        ],
        vec![2, 2],
    )?;

    let b = Tensor::from_vec_gpu(
        &device,
        vec![
            half::f16::from_f32(5.0),
            half::f16::from_f32(6.0),
            half::f16::from_f32(7.0),
            half::f16::from_f32(8.0),
        ],
        vec![2, 2],
    )?;

    println!("Matrix A: {:?}", a.to_vec_f32());
    println!("Matrix B: {:?}", b.to_vec_f32());

    let c = Tensor::einsum("ij,jk->ik", &[&a, &b])?;
    println!("einsum('ij,jk->ik'): {:?}", c.to_vec_f32());

    // Transpose via einsum
    let transposed = Tensor::einsum("ij->ji", &[&a])?;
    println!("einsum('ij->ji') (transpose): {:?}", transposed.to_vec_f32());

    // Trace via einsum
    let trace = Tensor::einsum("ii->", &[&a])?;
    println!("einsum('ii->') (trace): {:?}", trace.to_vec_f32());

    // Outer product via einsum
    let v1 = Tensor::from_vec_gpu(
        &device,
        vec![half::f16::from_f32(1.0), half::f16::from_f32(2.0)],
        vec![2],
    )?;

    let v2 = Tensor::from_vec_gpu(
        &device,
        vec![half::f16::from_f32(3.0), half::f16::from_f32(4.0)],
        vec![2],
    )?;

    let outer = Tensor::einsum("i,j->ij", &[&v1, &v2])?;
    println!("einsum('i,j->ij') (outer product): {:?}", outer.to_vec_f32());

    // Test Metal â†” Neural Engine conversion
    println!("\nTesting Metal â†” Neural Engine conversion...");

    use tensorlogic::device::{MetalBuffer, NeuralEngineBuffer, NeuralEngineOps};

    // Create Metal buffer
    let metal_data = vec![
        half::f16::from_f32(1.0),
        half::f16::from_f32(2.0),
        half::f16::from_f32(3.0),
        half::f16::from_f32(4.0),
    ];
    let metal_buf = MetalBuffer::from_f16_slice(device.metal_device(), &metal_data)?;
    println!("Metal buffer: {:?}", metal_buf.to_vec().iter().map(|x| x.to_f32()).collect::<Vec<_>>());

    // Convert Metal â†’ Neural Engine
    let ne_buf = metal_buf.to_neural_engine(&vec![2, 2])?;
    println!("Neural Engine buffer shape: {:?}", ne_buf.shape());
    println!("Neural Engine buffer data: {:?}", ne_buf.to_f16_vec().iter().map(|x| x.to_f32()).collect::<Vec<_>>());

    // Convert Neural Engine â†’ Metal (roundtrip)
    let metal_buf2 = ne_buf.to_metal_buffer(device.metal_device())?;
    println!("Roundtrip Metal buffer: {:?}", metal_buf2.to_vec().iter().map(|x| x.to_f32()).collect::<Vec<_>>());

    // Test Neural Engine operations
    println!("\nTesting Neural Engine operations...");
    println!("Neural Engine: {}", NeuralEngineOps::info());

    // Neural Engine matmul
    let ne_a = NeuralEngineBuffer::from_f16_slice(
        &vec![
            half::f16::from_f32(1.0),
            half::f16::from_f32(2.0),
            half::f16::from_f32(3.0),
            half::f16::from_f32(4.0),
        ],
        &[2, 2],
    )?;

    let ne_b = NeuralEngineBuffer::from_f16_slice(
        &vec![
            half::f16::from_f32(5.0),
            half::f16::from_f32(6.0),
            half::f16::from_f32(7.0),
            half::f16::from_f32(8.0),
        ],
        &[2, 2],
    )?;

    let ne_c = NeuralEngineOps::matmul(&ne_a, &ne_b, 2, 2, 2)?;
    println!("Neural Engine matmul result: {:?}", ne_c.to_f16_vec().iter().map(|x| x.to_f32()).collect::<Vec<_>>());

    // Neural Engine ReLU
    let ne_input = NeuralEngineBuffer::from_f16_slice(
        &vec![
            half::f16::from_f32(-2.0),
            half::f16::from_f32(-1.0),
            half::f16::from_f32(0.0),
            half::f16::from_f32(1.0),
            half::f16::from_f32(2.0),
        ],
        &[5],
    )?;

    let ne_relu = NeuralEngineOps::relu(&ne_input)?;
    println!("Neural Engine ReLU result: {:?}", ne_relu.to_f16_vec().iter().map(|x| x.to_f32()).collect::<Vec<_>>());

    println!("\nâœ… All operations completed successfully!");
    println!("   - Element-wise: add, sub, mul, div");
    println!("   - Activations: ReLU, GELU, Softmax");
    println!("   - Matrix ops: matmul");
    println!("   - Broadcasting: broadcast_to");
    println!("   - Reductions: sum, mean, max, min, sum_dim");
    println!("   - Einsum: ij,jk->ik, ij->ji, ii->, i,j->ij");
    println!("   - Neural Engine: Metal â†” CoreML conversion + operations");
    println!("   - All running on Metal GPU with f16 precision! ðŸš€");

    Ok(())
}

// Check magnitude of values at each layer
// Identify if there's scale drift or numerical precision issues

use tensorlogic::device::MetalDevice;
use tensorlogic::tensor::Tensor;
use std::path::Path;

fn tensor_stats(tensor: &Tensor, name: &str) {
    let values = tensor.to_vec();

    let mut min_val = f32::MAX;
    let mut max_val = f32::MIN;
    let mut sum = 0.0f32;
    let mut sum_sq = 0.0f32;

    for val in &values {
        let f = val.to_f32();
        if f < min_val { min_val = f; }
        if f > max_val { max_val = f; }
        sum += f;
        sum_sq += f * f;
    }

    let count = values.len() as f32;
    let mean = sum / count;
    let variance = (sum_sq / count) - (mean * mean);
    let std = variance.sqrt();

    println!("  {}: min={:.6}, max={:.6}, mean={:.6}, std={:.6}",
             name, min_val, max_val, mean, std);
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("=== Layer Magnitude Analysis ===\n");

    // This is a simplified test - we'll manually implement a few layers
    // to check if magnitudes drift

    let device = MetalDevice::new()?;

    // Create test input similar to an embedding
    // TinyLlama hidden_dim = 2048
    let hidden_dim = 2048;
    let seq_len = 1;

    // Initialize with reasonable values (similar to embeddings)
    let input_data: Vec<f32> = (0..hidden_dim)
        .map(|i| {
            // Simulate embedding values: small random-ish numbers
            let val = ((i as f32 * 0.01) % 2.0) - 1.0;  // Range: -1 to 1
            val * 0.1  // Scale down to -0.1 to 0.1 (typical embedding range)
        })
        .collect();

    let input_data_f16: Vec<half::f16> = input_data.iter()
        .map(|&x| half::f16::from_f32(x))
        .collect();

    let input = Tensor::from_vec_gpu(&device, input_data_f16.clone(), vec![seq_len, hidden_dim])?;

    println!("Input (simulated embedding):");
    tensor_stats(&input, "embedding");
    println!();

    // Test 1: RMSNorm behavior
    println!("Test 1: RMSNorm");

    // Create a simple normalization weight (all ones, like typical initialization)
    let norm_weight_data: Vec<f32> = vec![1.0; hidden_dim];
    let norm_weight_data_f16: Vec<half::f16> = norm_weight_data.iter()
        .map(|&x| half::f16::from_f32(x))
        .collect();
    let norm_weight = Tensor::from_vec_gpu(&device, norm_weight_data_f16, vec![hidden_dim])?;

    let normed = input.rms_norm(vec![hidden_dim], &norm_weight, 1e-5)?;
    tensor_stats(&normed, "after_rms_norm");
    println!();

    // Test 2: Linear projection behavior
    println!("Test 2: Linear Projection");

    // Create a simple weight matrix (small random values)
    let weight_data: Vec<f32> = (0..(hidden_dim * hidden_dim))
        .map(|i| {
            ((i as f32 * 0.001) % 1.0) - 0.5  // Range: -0.5 to 0.5
        })
        .collect();
    let weight_data_f16: Vec<half::f16> = weight_data.iter()
        .map(|&x| half::f16::from_f32(x * 0.01))  // Scale down by 0.01
        .collect();
    let weight = Tensor::from_vec_gpu(&device, weight_data_f16, vec![hidden_dim, hidden_dim])?;

    let proj = normed.matmul(&weight)?;
    tensor_stats(&proj, "after_matmul");
    println!();

    // Test 3: Residual addition
    println!("Test 3: Residual Addition");
    let residual = input.add(&proj)?;
    tensor_stats(&residual, "after_residual");
    println!();

    // Test 4: Multiple layers simulation
    println!("Test 4: Simulating Multiple Layers");
    println!("(Each iteration: norm → matmul → residual)");
    println!();

    let mut x = input.clone();

    for layer_idx in 0..22 {
        // Norm
        let normed = x.rms_norm(vec![hidden_dim], &norm_weight, 1e-5)?;

        // Project
        let proj = normed.matmul(&weight)?;

        // Residual
        x = x.add(&proj)?;

        // Print stats every 5 layers + first and last
        if layer_idx == 0 || layer_idx == 4 || layer_idx == 9 ||
           layer_idx == 14 || layer_idx == 19 || layer_idx == 21 {
            tensor_stats(&x, &format!("layer_{:02}", layer_idx));
        }
    }

    println!();
    println!("=== Analysis Complete ===");
    println!();
    println!("Expected behavior:");
    println!("  - Values should stay in reasonable range (e.g., -5 to +5)");
    println!("  - RMSNorm should keep std around 1.0");
    println!("  - Residuals should not cause values to grow/shrink unboundedly");
    println!();
    println!("If values drift (growing/shrinking), that's the cumulative error issue!");

    Ok(())
}

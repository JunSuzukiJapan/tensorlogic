//! Test Softmax determinism - run same operation 10 times
//! If Metal GPU softmax is deterministic, all results should be identical

use tensorlogic::tensor::Tensor;
use tensorlogic::device::MetalDevice;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("=== Softmax Determinism Test ===\n");
    println!("Testing if Metal GPU softmax produces identical results");
    println!("for the same input across multiple runs.\n");

    let device = MetalDevice::new()?;

    // Create test data - same as used in transformer attention
    // Attention scores shape: [seq_len, n_heads, seq_len]
    let seq_len = 2;
    let n_heads = 4;

    println!("Test: 3D Softmax (attention pattern)");
    println!("  Input: scaled_scores=[{}, {}, {}]", seq_len, n_heads, seq_len);
    println!("  Operation: softmax(scaled_scores) [applies to last dimension]\\n");

    // Create realistic attention scores (after scaling by 1/sqrt(head_dim))
    // Values should be in range of scaled dot products
    let scores_data: Vec<f32> = (0..(seq_len * n_heads * seq_len))
        .map(|i| {
            // Create varied scores that would result from Q @ K.T
            let val = (i as f32 * 0.5) % 5.0 - 2.5; // Range: -2.5 to 2.5
            val
        })
        .collect();

    let scores_data_f16: Vec<half::f16> = scores_data.iter()
        .map(|&x| half::f16::from_f32(x))
        .collect();

    // Run 10 times and collect results
    let mut all_results = Vec::new();

    for run in 0..10 {
        let scores = Tensor::from_vec_gpu(
            &device,
            scores_data_f16.clone(),
            vec![seq_len, n_heads, seq_len]
        )?;

        // Apply softmax (automatically applies to last dimension)
        let softmax_result = scores.softmax()?;
        let result_vec = softmax_result.to_vec();

        all_results.push(result_vec);

        if run == 0 {
            println!("  Run {}: First 5 values: {:.6}, {:.6}, {:.6}, {:.6}, {:.6}",
                run + 1,
                all_results[0][0].to_f32(),
                all_results[0][1].to_f32(),
                all_results[0][2].to_f32(),
                all_results[0][3].to_f32(),
                all_results[0][4].to_f32()
            );

            // Verify softmax properties: should sum to 1.0 along last dimension
            let first_head_probs = vec![
                all_results[0][0].to_f32(),
                all_results[0][1].to_f32(),
            ];
            let sum: f32 = first_head_probs.iter().sum();
            println!("  Softmax sum (should be ~1.0): {:.6}", sum);
        }
    }

    println!("\\nComparing all 10 runs...");

    let mut all_identical = true;
    let mut max_diff = 0.0f32;

    for run in 1..10 {
        for i in 0..all_results[0].len() {
            let diff = (all_results[0][i].to_f32() - all_results[run][i].to_f32()).abs();
            if diff > max_diff {
                max_diff = diff;
            }
            if diff > 0.0 {
                all_identical = false;
                if run == 1 && i < 5 {
                    println!("  Difference at index {}: run1={:.6}, run{}={:.6}, diff={:.6}",
                        i, all_results[0][i].to_f32(), run + 1, all_results[run][i].to_f32(), diff);
                }
            }
        }
    }

    println!("\\nResults:");
    println!("  Max difference across all runs: {:.10}", max_diff);

    if all_identical {
        println!("  ✅ DETERMINISTIC: All 10 runs produced identical results");
        println!("     Softmax is NOT the source of non-determinism");
    } else {
        println!("  ❌ NON-DETERMINISTIC: Results vary across runs");
        println!("     THIS IS THE BUG! Softmax is the source of non-determinism!");
    }

    println!("\\n=== Test Complete ===");

    Ok(())
}

// Grouped Query Attention (GQA) Implementation
// TinyLlama-style: 32 Q heads, 4 KV heads, head_dim=64

// Grouped Query Attention
// Q: [seq, num_q_heads * head_dim] = [seq, 2048]
// K: [seq, num_kv_heads * head_dim] = [seq, 256]
// V: [seq, num_kv_heads * head_dim] = [seq, 256]
fn grouped_query_attention(
    Q: float16[?, ?],
    K: float16[?, ?],
    V: float16[?, ?]
) -> float16[?, ?] {
    // TinyLlamaå›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    let seq_len = 4
    let num_q_heads = 32
    let num_kv_heads = 4
    let head_dim = 64
    // Group size: how many Q heads share one KV head
    let group_size = num_q_heads / num_kv_heads

    // 1. Reshape Q to [seq, num_q_heads, head_dim]
    let Q_heads = reshape(Q, [seq_len, num_q_heads, head_dim])

    // 2. Reshape K, V to [seq, num_kv_heads, head_dim]
    let K_heads = reshape(K, [seq_len, num_kv_heads, head_dim])
    let V_heads = reshape(V, [seq_len, num_kv_heads, head_dim])

    // 3. Expand K, V to match Q's head count
    // [seq, num_kv_heads, head_dim] -> [seq, num_kv_heads, 1, head_dim]
    let K_with_group = reshape(K_heads, [seq_len, num_kv_heads, 1, head_dim])
    let V_with_group = reshape(V_heads, [seq_len, num_kv_heads, 1, head_dim])

    // Broadcast: [seq, kv_heads, 1, head_dim] -> [seq, kv_heads, group_size, head_dim]
    let K_broadcast = broadcast_to(K_with_group, [seq_len, num_kv_heads, group_size, head_dim])
    let V_broadcast = broadcast_to(V_with_group, [seq_len, num_kv_heads, group_size, head_dim])

    // Reshape to final: [seq, num_q_heads, head_dim]
    let K_expanded = reshape(K_broadcast, [seq_len, num_q_heads, head_dim])
    let V_expanded = reshape(V_broadcast, [seq_len, num_q_heads, head_dim])

    // 4. Transpose for attention: [seq, heads, head_dim] -> [heads, seq, head_dim]
    // TensorLogic transpose currently only swaps last 2 dims, so we reshape manually
    // For simplicity, compute attention per-head and concatenate

    // Flatten heads for batch matmul: [seq, heads, head_dim] -> [seq*heads, head_dim]
    let Q_flat = reshape(Q_heads, [seq_len * num_q_heads, head_dim])
    let K_flat = reshape(K_expanded, [seq_len * num_q_heads, head_dim])
    let V_flat = reshape(V_expanded, [seq_len * num_q_heads, head_dim])

    // NOTE: å®Œå…¨ãªå®Ÿè£…ã«ã¯batched matmulãŒå¿…è¦
    // ã“ã“ã§ã¯ç°¡ç•¥ç‰ˆã‚’å®Ÿè£…ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ãŒå°ã•ã„å ´åˆï¼‰

    // ç°¡ç•¥ç‰ˆ: å…¨ä½“ã‚’1ã¤ã®å¤§ããªè¡Œåˆ—ã¨ã—ã¦æ‰±ã†
    // Q @ K^T: [seq, num_q_heads, head_dim] @ [seq, num_q_heads, head_dim]^T

    // å®Ÿéš›ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
    // Scale factor: 1/sqrt(head_dim) = 1/sqrt(64) = 0.125
    let scale = 0.125

    // Q_heads: [seq, heads, head_dim]
    // K_expanded: [seq, heads, head_dim]
    // scores = Q @ K^T per head

    // ç°¡å˜ã®ãŸã‚ã€å…¨ä½“ã‚’[seq*heads, head_dim]ã«å¤‰æ›ã—ã¦è¨ˆç®—
    let K_T = transpose(K_flat)  // [head_dim, seq*heads]
    let scores = matmul(Q_flat, K_T)  // [seq*heads, seq*heads]

    // Scale
    let scaled_scores = scores * scale

    // Softmax
    let attn_weights = softmax(scaled_scores, 1)

    // Apply attention to V
    let output = matmul(attn_weights, V_flat)  // [seq*heads, head_dim]

    // Reshape back: [seq*heads, head_dim] -> [seq, heads, head_dim] -> [seq, heads*head_dim]
    let output_heads = reshape(output, [seq_len, num_q_heads, head_dim])
    let result = reshape(output_heads, [seq_len, num_q_heads * head_dim])

    result := result
}

main {
    print("=== Grouped Query Attention (GQA) ãƒ†ã‚¹ãƒˆ ===")
    print("")

    // TinyLlama config
    let seq_len = 4
    let d_model = 2048
    let num_q_heads = 32
    let num_kv_heads = 4
    let head_dim = 64
    let group_size = 8  // 32 / 4

    print("æ§‹æˆ:")
    print("  seq_len:", seq_len)
    print("  d_model:", d_model)
    print("  num_q_heads:", num_q_heads)
    print("  num_kv_heads:", num_kv_heads)
    print("  head_dim:", head_dim)
    print("  group_size:", group_size, "(å„KVãƒ˜ãƒƒãƒ‰ã‚’", group_size, "å€‹ã®Qãƒ˜ãƒƒãƒ‰ã§å…±æœ‰)")
    print("")

    // Create dummy inputs
    print("ã‚¹ãƒ†ãƒƒãƒ—1: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ")
    let Q = ones([seq_len, num_q_heads * head_dim])  // [4, 2048]
    let K = ones([seq_len, num_kv_heads * head_dim]) // [4, 256]
    let V = ones([seq_len, num_kv_heads * head_dim]) // [4, 256]
    print("  Q:", shape(Q))
    print("  K:", shape(K))
    print("  V:", shape(V))
    print("")

    // Run GQA
    print("ã‚¹ãƒ†ãƒƒãƒ—2: GQAã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè¡Œ")
    let output = grouped_query_attention(Q, K, V)
    print("  å‡ºåŠ›:", shape(output))
    print("")

    print("==================================================")
    print("ğŸ‰ GQAã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…å®Œäº†!")
    print("")
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. TinyLlamaãƒ¢ãƒ‡ãƒ«é‡ã¿ã§ãƒ†ã‚¹ãƒˆ")
    print("  2. å®Œå…¨ãªTransformer Layerå®Ÿè£…")
    print("  3. 22å±¤ã‚¹ã‚¿ãƒƒã‚¯å®Ÿè¡Œ")
}

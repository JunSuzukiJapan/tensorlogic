# Local LLM Chat Implementation in TensorLogic

## æ¦‚è¦

TensorLogicã‚’ä½¿ç”¨ã—ã¦ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆTinyLlama 1.1Bï¼‰ã«ã‚ˆã‚‹ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

## ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³

### âœ… å®Œäº†ã—ãŸæ©Ÿèƒ½

1. **é…åˆ—ãƒªãƒ†ãƒ©ãƒ«å†…ã§ã®å¤‰æ•°ä½¿ç”¨**
   - æ§‹æ–‡: `ones([seq_len, d_model])`
   - ASTã€ãƒ‘ãƒ¼ã‚µãƒ¼ã€ã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã€å‹ãƒã‚§ãƒƒã‚«ãƒ¼ã‚’æ›´æ–°
   - ä¾‹: `examples/test_array_variables.tl`

2. **Transformer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å®Ÿè£…ï¼ˆTensorLogic DSLï¼‰**
   - Multi-Head Self-Attention: `examples/transformer_attention.tl`
   - Complete Transformer Block: `examples/transformer_block.tl`
   - LLM Inference Pipeline: `examples/llm_inference.tl`
   - ä½¿ç”¨å¯èƒ½ãªæ“ä½œ: `matmul`, `softmax`, `gelu`, `layer_norm`, `transpose`, `apply_attention_mask`, etc.

3. **ãƒ¢ãƒ‡ãƒ«ç®¡ç†**
   - TinyLlama 1.1B Chat (Q4_0) ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº† (609MB)
   - GGUFãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ€ãƒ¼å®Ÿè£…æ¸ˆã¿
   - ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: `~/.tensorlogic/models/tinyllama-1.1b-chat-q4_0.gguf`
   - ãƒ†ãƒ³ã‚½ãƒ«æ•°: 201å€‹

4. **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼**
   - HuggingFace tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨
   - `load_tokenizer(path)` - ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
   - `tokenize(tokenizer, text)` - ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
   - `detokenize(tokenizer, token_ids)` - ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒ‡ã‚³ãƒ¼ãƒ‰

5. **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥**
   - Top-k sampling: `top_k(logits, k)` - ä¸Šä½kå€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«é™å®š
   - Top-p (nucleus) sampling: `top_p(logits, p)` - ç´¯ç©ç¢ºç‡pã¾ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«é™å®š
   - `sample(probs)` - ç¢ºç‡åˆ†å¸ƒã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
   - ä¾‹: `examples/text_generation_sampling.tl` - å„æ‰‹æ³•ã®ãƒ‡ãƒ¢

6. **è‡ªå·±å›å¸°ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**
   - `examples/autoregressive_generation.tl` - å®Œå…¨ãªç”Ÿæˆãƒ«ãƒ¼ãƒ—ã®ãƒ‡ãƒ¢
   - ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã®ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼ˆ3ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
   - Top-k â†’ Top-p â†’ Softmax â†’ Sample ã®çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
   - æœ¬ç•ªç’°å¢ƒã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¾‹ï¼ˆãƒãƒ£ãƒƒãƒˆã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€å‰µä½œï¼‰
   - KV-Cacheã€Temperature scalingã€ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®èª¬æ˜

7. **ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹**
   - åŸºæœ¬çš„ãªãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¢: `examples/local_llm_chat.tl`
   - Chat REPL ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: `examples/chat_repl_demo.tl`
   - ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
   - ChatMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ
   - REPLã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè©³ç´°ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã€å…¥åŠ›å‡¦ç†ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼‰
   - ç‰¹æ®Šã‚³ãƒãƒ³ãƒ‰: /help, /clear, /exit, /config, /temp, /tokens
   - `generate(model, prompt, max_tokens, temperature)` APIï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰

### ğŸš§ å®Ÿè£…ä¸­ãƒ»æœªå®Ÿè£…ã®æ©Ÿèƒ½

1. **Transformer Forward Passï¼ˆRustã«ã‚ˆã‚‹å®Ÿè£…ãŒå¿…è¦ï¼‰**
   - Embeddingå±¤ã‹ã‚‰ã®å…¥åŠ›å‡¦ç†
   - è¤‡æ•°ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é †ä¼æ’­
   - æœ€çµ‚å±¤ã‹ã‚‰logitsã®ç”Ÿæˆ
   - GGUFãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰

2. **KV-Cache**
   - è‡ªå·±å›å¸°ç”Ÿæˆã®åŠ¹ç‡åŒ–
   - ä»¥å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®Key/Valueãƒ†ãƒ³ã‚½ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå®Ÿè£…

3. **Temperature Scaling**
   - `logits = logits / temperature` ã®å®Ÿè£…
   - generate()é–¢æ•°ã¸ã®çµ±åˆ

4. **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›**
   - ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡ºåŠ›
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®å‘ä¸Š
   - ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯/éåŒæœŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

5. **ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–REPLï¼ˆå®Œå…¨å®Ÿè£…ï¼‰**
   - å¯¾è©±å‹ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…
   - ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ç®¡ç†ã¨ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
   - ç‰¹æ®Šã‚³ãƒãƒ³ãƒ‰ã®å‡¦ç†
   - ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜/ãƒ­ãƒ¼ãƒ‰

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### TensorLogic DSLã«ã‚ˆã‚‹Transformerå®Ÿè£…

```tensorlogic
# Multi-Head Self-Attention (examples/transformer_attention.tl)
let seq_len = 8
let d_model = 512
let embeddings = positional_encoding(seq_len, d_model)

let Q = matmul(embeddings, W_q)
let K = matmul(embeddings, W_k)
let V = matmul(embeddings, W_v)

let scores = matmul(Q, transpose(K))
let mask = ones([seq_len, seq_len])
let masked_scores = apply_attention_mask(scores, mask)
let attn_weights = softmax(masked_scores, 1)
let output = matmul(attn_weights, V)
```

### ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã¨æ¨è«–

```tensorlogic
# Chat Application (examples/local_llm_chat.tl)
let model = load_model("/path/to/tinyllama-1.1b-chat-q4_0.gguf")
let response = generate(model, "What is the capital of Japan?", 100, 0.7)
```

## ä½¿ç”¨å¯èƒ½ãªæ“ä½œï¼ˆ48å€‹ï¼‰

### Tensor Creation (2)
- `zeros([m, n])` - ã‚¼ãƒ­ãƒ†ãƒ³ã‚½ãƒ«
- `ones([m, n])` - 1ã®ãƒ†ãƒ³ã‚½ãƒ«

### Shape Operations (6)
- `reshape(x, [new_shape])` - å½¢çŠ¶å¤‰æ›´
- `flatten(x)` - 1æ¬¡å…ƒåŒ–
- `transpose(x)` - è»¢ç½®
- `permute(x, [dims])` - æ¬¡å…ƒã®ä¸¦ã³æ›¿ãˆ
- `unsqueeze(x, dim)` - æ¬¡å…ƒè¿½åŠ 
- `squeeze(x, dim)` - æ¬¡å…ƒå‰Šé™¤

### Math Functions (6)
- `exp(x)`, `log(x)`, `sqrt(x)`
- `pow(x, y)`, `sin(x)`, `cos(x)`, `tan(x)`

### Aggregation (4)
- `max(x, dim)`, `min(x, dim)`
- `argmax(x, dim)`, `argmin(x, dim)`

### Activation (2)
- `gelu(x)` - GELUæ´»æ€§åŒ–
- `tanh(x)` - Tanhæ´»æ€§åŒ–

### Normalization (2)
- `layer_norm(x, shape, eps)` - Layer Normalization
- `batch_norm(x, mean, var, eps)` - Batch Normalization

### Attention/Masking (3)
- `apply_attention_mask(scores, mask)` - Attention maské©ç”¨
- `padding_mask(lengths, max_len)` - Padding maskç”Ÿæˆ
- `combine_masks(mask1, mask2)` - ãƒã‚¹ã‚¯ã®çµåˆ

### Broadcast (1)
- `broadcast_to(x, shape)` - ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ

### Fused Operations (4)
- `fused_add_relu(a, b)` - Add + ReLU
- `fused_mul_relu(a, b)` - Mul + ReLU
- `fused_affine(x, scale, bias)` - Affineå¤‰æ›
- `fused_gelu_linear(x, weight, bias)` - GELU + Linear

### ãã®ä»– (18)
- `matmul(a, b)` - è¡Œåˆ—ç©
- `softmax(x, dim)` - Softmax
- `relu(x)` - ReLU
- `positional_encoding(seq_len, d_model)` - ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
- ãªã©

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### Phase 1: Rustå®Ÿè£…ã«ã‚ˆã‚‹Transformer Forward Pass
1. Embeddingå±¤ã®å®Ÿè£…
2. Multi-Head Attentionã®æœ€é©åŒ–å®Ÿè£…
3. FFNã®å®Ÿè£…
4. Layer Normalizationã®çµ±åˆ

### Phase 2: æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…
1. KV-Cacheã®å®Ÿè£…
2. Top-k/Top-pã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
3. Temperature scalingã®å®Ÿè£…
4. `generate()` é–¢æ•°ã®å®Œå…¨å®Ÿè£…

### Phase 3: ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®å‘ä¸Š
1. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡ºåŠ›
2. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–REPL
3. ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ç®¡ç†
4. è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œ

### Phase 4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
1. Metal GPUã®å®Œå…¨æ´»ç”¨
2. Batchå‡¦ç†ã®ã‚µãƒãƒ¼ãƒˆ
3. ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–ã®æœ€é©åŒ–
4. ãƒ¡ãƒ¢ãƒªç®¡ç†ã®æ”¹å–„

## å‚è€ƒè³‡æ–™

- TinyLlama: https://github.com/jzhang38/TinyLlama
- GGUF Format: https://github.com/ggerganov/llama.cpp
- Attention is All You Need: https://arxiv.org/abs/1706.03762
- Tensor-Logic Paper: https://arxiv.org/abs/2510.12269

## å®Ÿè£…ä¾‹ã®å®Ÿè¡Œæ–¹æ³•

```bash
# Transformer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ‡ãƒ¢
./target/debug/tl run examples/transformer_attention.tl
./target/debug/tl run examples/transformer_block.tl
./target/debug/tl run examples/llm_inference.tl

# é…åˆ—ãƒªãƒ†ãƒ©ãƒ«å†…ã§ã®å¤‰æ•°ä½¿ç”¨ã®ãƒ†ã‚¹ãƒˆ
./target/debug/tl run examples/test_array_variables.tl

# ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¢
./target/debug/tl run examples/local_llm_chat.tl
```

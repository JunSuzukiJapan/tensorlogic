# Руководство по Квантованным Моделям GGUF

Этот документ объясняет, как загружать и использовать квантованные модели в формате GGUF (совместимые с llama.cpp) в TensorLogic.

## О Формате GGUF

GGUF (GGML Universal Format) — это эффективный формат квантования для больших языковых моделей, разработанный проектом llama.cpp.

### Основные Характеристики

- Эффективность памяти через квантование 4-bit/8-bit (до 8x сжатия)
- Блочное квантование сохраняет точность
- Совместимость с llama.cpp, Ollama, LM Studio и другими

### Форматы Квантования, Поддерживаемые TensorLogic

- ✅ **Q4_0**: Квантование 4-bit (максимальное сжатие)
- ✅ **Q8_0**: Квантование 8-bit (баланс точности и сжатия)
- ✅ **F16**: 16-bit плавающая точка (высокая точность)
- ✅ **F32**: 32-bit плавающая точка (максимальная точность)

## Основное Использование

### 1. Загрузить Квантованную Модель

Автоматически деквантизируется в f16 и загружается в GPU Metal:

```tensorlogic
model = load_model("models/llama-7b-q4_0.gguf")
```

### 2. Получить Тензоры из Модели

```tensorlogic
embeddings = model.get_tensor("token_embd.weight")
output_weight = model.get_tensor("output.weight")
```

## Выбор Формата Квантования

### Q4_0 (4-bit)

- **Память**: Минимальное использование (~1/8 от исходной модели)
- **Скорость**: Самый быстрый вывод
- **Точность**: Небольшая деградация (обычно приемлемая)
- **Применение**: Чат-боты, общая генерация текста

### Q8_0 (8-bit)

- **Память**: Умеренное использование (~1/4 от исходной модели)
- **Скорость**: Быстро
- **Точность**: Высокая (почти эквивалентна F16)
- **Применение**: Высококачественная генерация, помощники по программированию

### F16 (16-bit)

- **Память**: ~1/2 от исходной модели
- **Скорость**: Стандартная
- **Точность**: Нативный формат TensorLogic, оптимизирован для GPU Metal
- **Применение**: Когда требуется максимальное качество

## Практический Пример: Embeddings Токенов

```tensorlogic
// Получить embeddings токенов из модели LLama
embedding_table = model.get_tensor("token_embd.weight")
print("Embedding shape:", embedding_table.shape)  // [vocab_size, hidden_dim]

// Получить вектор embedding из ID токена
fn get_token_embedding(embedding_table: float16[V, D],
                             token_id: int) -> float16[D] {
    return embedding_table[token_id, :]
}
```

## Экономия Памяти от Квантования

Пример: Модель LLama-7B (7 миллиардов параметров):

| Формат     | Использование Памяти | Сжатие |
|------------|---------------------|--------|
| F32 (исх.) | ~28 GB              | 1x     |
| F16        | ~14 GB              | 2x     |
| Q8_0       | ~7 GB               | 4x     |
| Q4_0       | ~3.5 GB             | 8x     |

TensorLogic конвертирует все форматы в f16 при загрузке и эффективно выполняет их на GPU Metal.

## Загрузка и Установка Моделей

### 1. Загрузить Модели GGUF с HuggingFace

Пример: https://huggingface.co/TheBloke

### 2. Рекомендуемые Модели (для начинающих)

- **TinyLlama-1.1B-Chat-v1.0** (Q4_0: ~600MB)
- **Phi-2** (Q4_0: ~1.6GB)
- **Mistral-7B** (Q4_0: ~3.8GB)

### 3. Загрузить в TensorLogic

```tensorlogic
model = load_model("path/to/model-q4_0.gguf")
```

## Важные Замечания

- Квантованные модели только для чтения (нельзя сохранить из TensorLogic)
- Используйте неквантованные модели (F16/F32) для обучения
- Q4/Q8 оптимизированы только для вывода
- Все форматы квантования автоматически деквантизируются в f16 и загружаются на GPU

## Ссылки

- [Спецификация GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [Модели GGUF на HuggingFace](https://huggingface.co/TheBloke)
- [Руководство по Загрузке Моделей](model_loading.md)

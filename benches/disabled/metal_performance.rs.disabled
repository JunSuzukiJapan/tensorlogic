use half::f16;
//! Metal GPU Performance Benchmark
//!
//! Comprehensive benchmark for Metal GPU operations including:
//! - Matrix multiplication (various sizes)
//! - Element-wise operations
//! - Reduction operations
//! - Memory transfer bandwidth
//! - Kernel launch overhead

use std::time::Instant;
use tensorlogic::device::MetalDevice;
use tensorlogic::tensor::Tensor;

/// Benchmark result with detailed metrics
#[derive(Debug)]
struct BenchmarkResult {
    operation: String,
    size: String,
    avg_time_ms: f64,
    min_time_ms: f64,
    max_time_ms: f64,
    gflops: f64,
    bandwidth_gbps: f64,
}

impl BenchmarkResult {
    fn print_header() {
        println!("{:<30} {:<15} {:>12} {:>12} {:>12} {:>12} {:>12}",
            "Operation", "Size", "Avg (ms)", "Min (ms)", "Max (ms)", "GFLOPS", "GB/s");
        println!("{}", "-".repeat(110));
    }

    fn print(&self) {
        println!("{:<30} {:<15} {:>12.4} {:>12.4} {:>12.4} {:>12.2} {:>12.2}",
            self.operation,
            self.size,
            self.avg_time_ms,
            self.min_time_ms,
            self.max_time_ms,
            self.gflops,
            self.bandwidth_gbps
        );
    }
}

fn benchmark_operation<F>(
    name: &str,
    size: &str,
    iterations: usize,
    warmup: usize,
    flops: f64,
    bytes: f64,
    mut op: F,
) -> BenchmarkResult
where
    F: FnMut(),
{
    // Warmup
    for _ in 0..warmup {
        op();
    }

    // Benchmark
    let mut times = Vec::with_capacity(iterations);
    for _ in 0..iterations {
        let start = Instant::now();
        op();
        let duration = start.elapsed();
        times.push(duration.as_secs_f64());
    }

    let avg_time = times.iter().sum::<f64>() / iterations as f64;
    let min_time = times.iter().cloned().fold(f64::INFINITY, f64::min);
    let max_time = times.iter().cloned().fold(0.0f64, f64::max);

    let gflops = if flops > 0.0 { flops / (avg_time * 1e9) } else { 0.0 };
    let bandwidth_gbps = if bytes > 0.0 { bytes / (avg_time * 1e9) } else { 0.0 };

    BenchmarkResult {
        operation: name.to_string(),
        size: size.to_string(),
        avg_time_ms: avg_time * 1000.0,
        min_time_ms: min_time * 1000.0,
        max_time_ms: max_time * 1000.0,
        gflops,
        bandwidth_gbps,
    }
}

fn benchmark_matmul(device: &MetalDevice) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();
    let sizes = vec![64, 128, 256, 512, 1024];

    println!("\n=== Matrix Multiplication Benchmark ===");
    BenchmarkResult::print_header();

    for size in sizes {
        let a = Tensor::ones(device, vec![size, size]).unwrap();
        let b = Tensor::ones(device, vec![size, size]).unwrap();

        let flops = 2.0 * size.pow(3) as f64; // 2*N^3 FLOPs for NxN matmul
        let bytes = 3.0 * size.pow(2) as f64 * 2.0; // 3 matrices * N^2 * 2 bytes (f16)

        let result = benchmark_operation(
            "MatMul",
            &format!("{}x{}", size, size),
            100,
            10,
            flops,
            bytes,
            || {
                let _ = a.matmul(&b).unwrap();
            },
        );

        result.print();
        results.push(result);
    }

    results
}

fn benchmark_elementwise(device: &MetalDevice) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();
    let sizes = vec![1000, 10000, 100000, 1000000];

    println!("\n=== Element-wise Operations Benchmark ===");
    BenchmarkResult::print_header();

    for size in sizes {
        let a = Tensor::ones(device, vec![size]).unwrap();
        let b = Tensor::ones(device, vec![size]).unwrap();

        let flops = size as f64; // 1 FLOP per element
        let bytes = 3.0 * size as f64 * 2.0; // 3 tensors * size * 2 bytes

        // Addition
        let result = benchmark_operation(
            "Add",
            &format!("{}", size),
            100,
            10,
            flops,
            bytes,
            || {
                let _ = a.add(&b);
            },
        );
        result.print();
        results.push(result);

        // Multiplication
        let result = benchmark_operation(
            "Mul",
            &format!("{}", size),
            100,
            10,
            flops,
            bytes,
            || {
                let _ = a.mul(&b);
            },
        );
        result.print();
        results.push(result);
    }

    results
}

fn benchmark_reduction(device: &MetalDevice) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();
    let sizes = vec![1000, 10000, 100000, 1000000];

    println!("\n=== Reduction Operations Benchmark ===");
    BenchmarkResult::print_header();

    for size in sizes {
        let a = Tensor::ones(device, vec![size]).unwrap();

        let flops = size as f64; // N additions
        let bytes = size as f64 * 2.0; // Read all elements

        let result = benchmark_operation(
            "Sum",
            &format!("{}", size),
            100,
            10,
            flops,
            bytes,
            || {
                let _ = a.sum();
            },
        );
        result.print();
        results.push(result);
    }

    results
}

fn benchmark_memory_transfer(device: &MetalDevice) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();
    let sizes = vec![1024, 10240, 102400, 1024000];

    println!("\n=== Memory Transfer Bandwidth Benchmark ===");
    BenchmarkResult::print_header();

    for size in sizes {
        // Host to Device
        let data: Vec<half::f16> = vec![half::f16::from_f32(1.0); size];
        let bytes = size as f64 * 2.0;

        let result = benchmark_operation(
            "Host→Device",
            &format!("{}", size),
            100,
            10,
            0.0,
            bytes,
            || {
                let _ = Tensor::from_vec(data.clone(), vec![size]).unwrap();
            },
        );
        result.print();
        results.push(result);

        // Device to Host
        let tensor = Tensor::ones(device, vec![size]).unwrap();

        let result = benchmark_operation(
            "Device→Host",
            &format!("{}", size),
            100,
            10,
            0.0,
            bytes,
            || {
                let _ = tensor.to_vec();
            },
        );
        result.print();
        results.push(result);
    }

    results
}

fn benchmark_activation_functions(device: &MetalDevice) -> Vec<BenchmarkResult> {
    let mut results = Vec::new();
    let sizes = vec![1000, 10000, 100000, 1000000];

    println!("\n=== Activation Functions Benchmark ===");
    BenchmarkResult::print_header();

    for size in sizes {
        let a = Tensor::ones(device, vec![size]).unwrap();
        let bytes = 2.0 * size as f64 * 2.0; // Read + Write

        // ReLU
        let result = benchmark_operation(
            "ReLU",
            &format!("{}", size),
            100,
            10,
            size as f64, // 1 comparison per element
            bytes,
            || {
                let _ = a.relu();
            },
        );
        result.print();
        results.push(result);

        // GELU
        let result = benchmark_operation(
            "GELU",
            &format!("{}", size),
            100,
            10,
            8.0 * size as f64, // ~8 ops per element (tanh approximation)
            bytes,
            || {
                let _ = a.gelu();
            },
        );
        result.print();
        results.push(result);
    }

    results
}

fn print_summary(all_results: &[Vec<BenchmarkResult>]) {
    println!("\n\n=== Performance Summary ===\n");

    // Find best performances
    let mut max_gflops = 0.0f64;
    let mut max_bandwidth = 0.0f64;

    for results in all_results {
        for r in results {
            if r.gflops > max_gflops {
                max_gflops = r.gflops;
            }
            if r.bandwidth_gbps > max_bandwidth {
                max_bandwidth = r.bandwidth_gbps;
            }
        }
    }

    println!("Peak Compute Performance: {:.2} GFLOPS", max_gflops);
    println!("Peak Memory Bandwidth:    {:.2} GB/s", max_bandwidth);

    // Calculate average latencies
    println!("\nAverage Latencies:");
    for results in all_results {
        if !results.is_empty() {
            let avg = results.iter().map(|r| r.avg_time_ms).sum::<f64>() / results.len() as f64;
            println!("  {}: {:.4} ms", results[0].operation, avg);
        }
    }
}

fn main() {
    println!("╔═══════════════════════════════════════════════════╗");
    println!("║   TensorLogic Metal GPU Performance Benchmark    ║");
    println!("╚═══════════════════════════════════════════════════╝");

    let device = MetalDevice::new().expect("Failed to create Metal device");
    println!("\nMetal Device: {}", device.name());
    println!("Using half-precision (f16) floating point\n");

    let mut all_results = Vec::new();

    all_results.push(benchmark_matmul(&device));
    all_results.push(benchmark_elementwise(&device));
    all_results.push(benchmark_reduction(&device));
    all_results.push(benchmark_memory_transfer(&device));
    all_results.push(benchmark_activation_functions(&device));

    print_summary(&all_results);

    println!("\n\n=== Benchmark Complete ===");
    println!("Save these results for comparison after optimization!\n");
}

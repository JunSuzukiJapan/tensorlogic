# TensorLogic æ€§èƒ½æœ€é©åŒ–è¨ˆç”»
## Candleæ¯”è¼ƒåˆ†æã«åŸºã¥ãåŒ…æ‹¬çš„ä¿®æ­£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

**ä½œæˆæ—¥**: 2025-11-01
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.1
**æœ€çµ‚æ›´æ–°**: 2025-11-01
**å¯¾è±¡**: TensorLogic v0.1.5
**ç›®æ¨™**: Candleã¨ã®æ€§èƒ½ã‚®ãƒ£ãƒƒãƒ—ï¼ˆç¾åœ¨3000å€ï¼‰ã‚’10å€ä»¥å†…ã«ç¸®å°

## ğŸ“ å®Ÿè£…è¨˜éŒ²

### 2025-11-01: Phase 1 å®Ÿè£…é–‹å§‹

#### âœ… å®Œäº†ã—ãŸæœ€é©åŒ–

**Problem 5: ã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—åŠ **
- **å¤‰æ›´**: `src/device/commands.rs` - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒƒãƒã‚µã‚¤ã‚º 50 â†’ 500
- **ã‚³ãƒŸãƒƒãƒˆ**: f97f749
- **åŠ¹æœ**: ãƒ•ãƒ©ãƒƒã‚·ãƒ¥å›æ•°å‰Šæ¸›ï¼ˆ20å›/ãƒˆãƒ¼ã‚¯ãƒ³ â†’ 2å›/ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
- **æ¸¬å®šçµæœ**: å˜ç‹¬ã§ã¯æ€§èƒ½æ”¹å–„ãŒè¦³æ¸¬ã•ã‚Œãšï¼ˆä¾ç„¶ã¨ã—ã¦120ç§’/ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
- **åˆ¤å®š**: æ­£å¸¸ã«å‹•ä½œã™ã‚‹ãŒã€ä»–ã®å•é¡Œã®å½±éŸ¿ãŒå¤§ãã™ãã¦åŠ¹æœãŒéš ã‚Œã¦ã„ã‚‹

#### âŒ å¤±æ•—ã—ãŸæœ€é©åŒ–è©¦è¡Œ

**Problem 1: Tensorä½œæˆæ™‚GPUåŒæœŸå‰Šé™¤ - 3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦è¡Œ**

**è©¦è¡Œ1: å®Œå…¨å‰Šé™¤**
- **å¤‰æ›´**: `src/tensor/tensor_creation.rs` - `sync_and_read()`å‘¼ã³å‡ºã—ã‚’å‰Šé™¤
- **çµæœ**: ãƒ‡ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ¼ã‚ºã§ãƒãƒ³ã‚°ï¼ˆprefillã¯å®Œäº†ï¼‰
- **åŸå› **: ã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ã®é©åˆ‡ãªãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãŒè¡Œã‚ã‚Œãšã€GPUæ“ä½œãŒå®Œäº†ã‚’å¾…æ©Ÿ

**è©¦è¡Œ2: flush_gpu()ã¸ã®ç½®æ›**
```rust
// æ–°è¦ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ : src/tensor/tensor_io.rs
fn flush_gpu(&self) -> TensorResult<()> {
    if let Device::Metal(ref device) = self.device() {
        device.wait_until_completed()?;  // â† ã“ã“ãŒå•é¡Œ
    }
    Ok(())
}
```
- **æ„å›³**: GPUâ†’CPUãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’é¿ã‘ã¤ã¤ã€ã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ã®ã¿ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
- **çµæœ**: ä¾ç„¶ã¨ã—ã¦120ç§’/ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ”¹å–„ãªã—ï¼‰
- **åŸå› **: `wait_until_completed()`ãŒ1,100+å›/ãƒˆãƒ¼ã‚¯ãƒ³å‘¼ã°ã‚Œã€åŒæœŸå¾…æ©ŸãŒç™ºç”Ÿ
- **æ¸¬å®š**: å€‹åˆ¥ã®rms_normæ“ä½œã¯0.068ms â†’ 0.011msï¼ˆ6å€é«˜é€ŸåŒ–ï¼‰ã—ãŸãŒã€å…¨ä½“æ€§èƒ½ã¯ä¸å¤‰

**è©¦è¡Œ3: å†åº¦å®Œå…¨å‰Šé™¤ï¼ˆSharedMemoryä»®èª¬ï¼‰**
- **ä»®èª¬**: Metal SharedMemoryãƒ¢ãƒ¼ãƒ‰ã§ã¯CPUæ›¸ãè¾¼ã¿ãŒå³åº§ã«GPUã‹ã‚‰å¯è¦–
- **çµæœ**: å†ã³ãƒãƒ³ã‚°
- **åˆ¤å®š**: ã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ã®æ˜ç¤ºçš„ç®¡ç†ãŒä¾ç„¶ã¨ã—ã¦å¿…è¦

#### ğŸ” æ ¹æœ¬å•é¡Œã®åˆ†æ

**å•é¡Œã®æœ¬è³ª**:
```
sync_and_read() = GPUâ†’CPUè»¢é€ (é«˜ã‚³ã‚¹ãƒˆ) + wait_until_completed() (åŒæœŸå¾…æ©Ÿ)
                  â†‘ å‰Šé™¤ã—ãŸã„              â†‘ å‰Šé™¤ã™ã‚‹ã¨ãƒãƒ³ã‚°
```

**ã‚¸ãƒ¬ãƒ³ãƒ**:
- `sync_and_read()`ã‚’å‰Šé™¤ â†’ ãƒãƒ³ã‚°ï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡æœªå®Œäº†ï¼‰
- `flush_gpu()`ã«ç½®æ› â†’ åŒæœŸå¾…æ©Ÿã¯æ®‹ã‚‹ï¼ˆæ€§èƒ½æ”¹å–„ãªã—ï¼‰
- å®Œå…¨å‰Šé™¤ â†’ ãƒãƒ³ã‚°ï¼ˆä¾å­˜é–¢ä¿‚ã®ç ´å£Šï¼‰

**Candleã¨ã®é•ã„**:
Candleã¯åŒæ§˜ã®åŒæœŸãªã—ã§å‹•ä½œã—ã¦ã„ã¾ã™ã€‚ã“ã®å·®ç•°ã®åŸå› ã¨ã—ã¦è€ƒãˆã‚‰ã‚Œã‚‹ã‚‚ã®ï¼š
1. Candleã®ã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ç®¡ç†æˆ¦ç•¥ãŒç•°ãªã‚‹å¯èƒ½æ€§
2. TensorLogicã®ä¾å­˜é–¢ä¿‚ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§
3. Metal APIã®ä½¿ç”¨æ–¹æ³•ã«æ ¹æœ¬çš„ãªé•ã„ãŒã‚ã‚‹å¯èƒ½æ€§

#### ğŸ“Š æ¸¬å®šãƒ‡ãƒ¼ã‚¿

| æœ€é©åŒ– | rms_normæ™‚é–“ | 1ãƒˆãƒ¼ã‚¯ãƒ³æ™‚é–“ | çŠ¶æ…‹ |
|--------|--------------|---------------|------|
| ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | 3.450ms | 90ç§’ | æ­£å¸¸å‹•ä½œ |
| ãƒãƒƒãƒã‚µã‚¤ã‚º500 | 0.011ms | 120ç§’ | æ­£å¸¸å‹•ä½œ |
| flush_gpu() | 0.008ms | 120ç§’ | æ­£å¸¸å‹•ä½œ |
| å®Œå…¨å‰Šé™¤ | 0.006ms | ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ | ãƒãƒ³ã‚° |

**è€ƒå¯Ÿ**:
- å€‹åˆ¥ã®GPUæ“ä½œã¯å¤§å¹…ã«é«˜é€ŸåŒ–ï¼ˆ50å€ä»¥ä¸Šï¼‰
- ã—ã‹ã—å…¨ä½“æ€§èƒ½ã¯æ”¹å–„ã›ãšã€ã‚€ã—ã‚æ‚ªåŒ–
- åŒæœŸå¾…æ©ŸãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã ãŒã€å‰Šé™¤ã™ã‚‹ã¨æ­£ç¢ºæ€§ãŒå¤±ã‚ã‚Œã‚‹

#### ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**çŸ­æœŸ**ï¼ˆæ¬¡å›ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰:
1. Problem 2ï¼ˆshapeãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼‰ã‚’å®Ÿè£…
2. Problem 4ï¼ˆRoPEä¸è¦ã‚¯ãƒ­ãƒ¼ãƒ³å‰Šé™¤ï¼‰ã‚’å®Ÿè£…
3. ã“ã‚Œã‚‰ã¯åŒæœŸã«ä¾å­˜ã—ãªã„æœ€é©åŒ–

**ä¸­æœŸ**ï¼ˆè¦èª¿æŸ»ï¼‰:
1. Candleã®ã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ç®¡ç†ã‚’è©³ç´°ã«èª¿æŸ»
2. Metalä¾å­˜é–¢ä¿‚ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°APIã®èª¿æŸ»
3. `enqueue` vs `commit`ã®ä½¿ã„åˆ†ã‘ã‚’å†æ¤œè¨

**é•·æœŸ**:
Problem 1ã¯å˜ç´”ãªå‰Šé™¤ã§ã¯è§£æ±ºã§ããšã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¬ãƒ™ãƒ«ã®å†è¨­è¨ˆãŒå¿…è¦

---

## ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### ç¾çŠ¶
- **ç¾åœ¨ã®æ€§èƒ½**: ~0.011 tok/s (90ç§’/ãƒˆãƒ¼ã‚¯ãƒ³)
- **Candleã®æ€§èƒ½**: ~36 tok/s
- **æ€§èƒ½å·®**: ç´„3000å€

### æ ¹æœ¬åŸå› 
Candleã¨ã®è©³ç´°æ¯”è¼ƒã«ã‚ˆã‚Šã€ä»¥ä¸‹5ã¤ã®ä¸»è¦å•é¡Œã‚’ç‰¹å®šï¼š

1. **Tensorä½œæˆæ™‚ã®å¼·åˆ¶GPUåŒæœŸ** (1,100+å›/ãƒˆãƒ¼ã‚¯ãƒ³) â†’ 55-110ms
2. **O(NÂ²) KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†** (44å›/ãƒˆãƒ¼ã‚¯ãƒ³) â†’ 22-44ms
3. **ShapeæŠ½å‡ºã§ã®GPUåŒæœŸ** (22å›/ãƒˆãƒ¼ã‚¯ãƒ³) â†’ 0.44-1.1ms
4. **RoPEã§ã®ä¸è¦ãªã‚¯ãƒ­ãƒ¼ãƒ³** (44+å›/ãƒˆãƒ¼ã‚¯ãƒ³) â†’ 8.8-22ms
5. **å°ã•ãªã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ãƒãƒƒãƒ** (20ãƒ•ãƒ©ãƒƒã‚·ãƒ¥/ãƒˆãƒ¼ã‚¯ãƒ³) â†’ 2-6ms

**åˆè¨ˆã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: 90-187ms/ãƒˆãƒ¼ã‚¯ãƒ³ (å®Ÿè¨ˆç®—ã¯10-20ms)

### æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„
- **Phase 1 (å³åº§)**: 80%æ”¹å–„ â†’ ~18ms/ãƒˆãƒ¼ã‚¯ãƒ³ (55 tok/s)
- **Phase 2 (ä¸­æœŸ)**: 95%æ”¹å–„ â†’ ~8ms/ãƒˆãƒ¼ã‚¯ãƒ³ (125 tok/s)
- **Phase 3 (é•·æœŸ)**: CandleåŒç­‰ â†’ ~3-6ms/ãƒˆãƒ¼ã‚¯ãƒ³ (166-333 tok/s)

---

## Phase 1: å³åº§ã®æ”¹å–„ (80%æ€§èƒ½å‘ä¸Š)

### å•é¡Œ1: Tensorä½œæˆæ™‚ã®å¼·åˆ¶GPUåŒæœŸ

#### ç¾çŠ¶åˆ†æ

**å ´æ‰€**: `src/tensor/tensor_creation.rs:148-156`

```rust
pub fn from_vec_gpu(data: Vec<T>, shape: Vec<usize>, pool: Arc<Mutex<BufferPool>>) -> TensorResult<Self> {
    // ... GPU bufferä½œæˆ ...

    let tensor = Self::new(buffer, shape, Device::Metal(device.clone()))?;

    // Force synchronization to ensure GPU buffer is fully initialized
    // This prevents race conditions when the tensor is used immediately after creation
    use crate::tensor::TensorIO;
    let _ = tensor.sync_and_read();  // â† å•é¡Œ: å…¨ãƒ‡ãƒ¼ã‚¿ã‚’GPUã‹ã‚‰CPUã«èª­ã¿è¾¼ã‚€ï¼

    Ok(tensor)
}
```

**å•é¡Œç‚¹**:
- GPUâ†’CPUã¸ã®å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿è»¢é€ï¼ˆãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…ã®ç„¡é§„ï¼‰
- GPUæ“ä½œå®Œäº†ã¾ã§å¾…æ©Ÿï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–ã®é˜»å®³ï¼‰
- 1ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã§1,100+å›ç™ºç”Ÿï¼ˆå„ç·šå½¢æŠ•å½±ã€embeddingã€ä¸­é–“çµæœï¼‰

**æ€§èƒ½å½±éŸ¿**: 55-110ms/ãƒˆãƒ¼ã‚¯ãƒ³

#### ä¿®æ­£æ–¹æ³•

**Option A: å®Œå…¨å‰Šé™¤** (æ¨å¥¨)

```rust
pub fn from_vec_gpu(data: Vec<T>, shape: Vec<usize>, pool: Arc<Mutex<BufferPool>>) -> TensorResult<Self> {
    // ... GPU bufferä½œæˆ ...

    let tensor = Self::new(buffer, shape, Device::Metal(device.clone()))?;

    // å‰Šé™¤: let _ = tensor.sync_and_read();

    Ok(tensor)
}
```

**æ ¹æ‹ **:
- Metalã®ã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ã¯é †åºã‚’ä¿è¨¼
- æ˜ç¤ºçš„ãªä¾å­˜é–¢ä¿‚ï¼ˆåŒã˜command bufferå†…ï¼‰ã§åŒæœŸã¯ä¸è¦
- Candleã‚‚åŒæœŸãªã—ã§å‹•ä½œ

**Option B: æ¡ä»¶ä»˜ãåŒæœŸ** (ãƒ‡ãƒãƒƒã‚°ç”¨)

```rust
pub fn from_vec_gpu(data: Vec<T>, shape: Vec<usize>, pool: Arc<Mutex<BufferPool>>) -> TensorResult<Self> {
    // ... GPU bufferä½œæˆ ...

    let tensor = Self::new(buffer, shape, Device::Metal(device.clone()))?;

    // ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ã®ã¿åŒæœŸ
    if cfg!(debug_assertions) && std::env::var("TL_VERIFY_GPU_INIT").is_ok() {
        use crate::tensor::TensorIO;
        let _ = tensor.sync_and_read();
    }

    Ok(tensor)
}
```

#### å®Ÿè£…æ‰‹é †

**ã‚¹ãƒ†ãƒƒãƒ—1**: from_vec_gpu_pooledä¿®æ­£
```bash
# ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†
vim src/tensor/tensor_creation.rs
# 155è¡Œç›®ã®sync_and_read()ã‚’å‰Šé™¤
```

**ã‚¹ãƒ†ãƒƒãƒ—2**: from_vec_gpuã‚‚ä¿®æ­£
```rust
// src/tensor/tensor_creation.rs:75ä»˜è¿‘
pub fn from_vec_gpu(data: Vec<T>, shape: Vec<usize>, device: MetalDevice) -> TensorResult<Self> {
    // ...
    let tensor = Self::new(buffer, shape, Device::Metal(device.clone()))?;
    // å‰Šé™¤: let _ = tensor.sync_and_read();
    Ok(tensor)
}
```

**ã‚¹ãƒ†ãƒƒãƒ—3**: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
```bash
# åŸºæœ¬ãƒ†ã‚¹ãƒˆ
cargo test tensor_creation

# çµ±åˆãƒ†ã‚¹ãƒˆ
cargo test --release ops::tests

# ãƒãƒ£ãƒƒãƒˆãƒ‡ãƒ¢
timeout 30 ./target/release/tl run examples/chat_2layers_f32.tl
```

#### ãƒªã‚¹ã‚¯è©•ä¾¡

| ãƒªã‚¹ã‚¯ | ç¢ºç‡ | å½±éŸ¿ | å¯¾ç­– |
|--------|------|------|------|
| GPUåˆæœŸåŒ–race condition | ä½ | é«˜ | Metalã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ä¾å­˜é–¢ä¿‚ã‚’æ¤œè¨¼ |
| æ—¢å­˜ãƒ†ã‚¹ãƒˆå¤±æ•— | ä¸­ | ä¸­ | æ®µéšçš„rolloutã€ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ä¿æŒ |
| ä¸æ­£ç¢ºãªçµæœ | ä½ | é«˜ | æ•°å€¤ç²¾åº¦ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ å®Ÿè¡Œ |

**ç·©å’Œç­–**:
- ãƒ‡ãƒãƒƒã‚°ãƒ“ãƒ«ãƒ‰ã§ã¯åŒæœŸã‚’ä¿æŒ
- ç’°å¢ƒå¤‰æ•°`TL_VERIFY_GPU_INIT=1`ã§å¼·åˆ¶åŒæœŸãƒ¢ãƒ¼ãƒ‰
- CI/CDã§è‡ªå‹•å›å¸°ãƒ†ã‚¹ãƒˆ

#### æœŸå¾…åŠ¹æœ

- **æ€§èƒ½æ”¹å–„**: 50-100ms/ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š**: 0.011 â†’ 0.2-0.5 tok/s (18-45å€)

---

### å•é¡Œ2: Shapeãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®GPUåŒæœŸ

#### ç¾çŠ¶åˆ†æ

**å‘¼ã³å‡ºã—ç®‡æ‰€** (ä¾‹: `examples/chat_full_22layers_f16.tl`):

```rust
// Line 151-152: Prefill
let x_shp = shape(x)
let seq_len = x_shp[0]

// Lines 333-557: Decode (22å±¤ Ã— 2å› = 44å›)
let KV0_shp = shape(KV0)
let cache_len = KV0_shp[0]
```

**shape()ã®å®Ÿè£…** (`src/interpreter/builtin_tensor.rs`ä»˜è¿‘):

```rust
fn eval_shape(&mut self, args: &[TensorExpr]) -> RuntimeResult<Value> {
    let val = self.eval_expr(&args[0])?;
    match val {
        Value::TensorF32(ref t) => {
            let dims = t.dims();  // â† GPUåŒæœŸãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§
            // ...
        }
    }
}
```

**Tensorã®ç¾çŠ¶**:

```rust
// src/tensor/tensor.rs
pub struct Tensor<T: FloatType> {
    pub(crate) shape: TensorShape,  // shapeæƒ…å ±ã¯æ—¢ã«æŒã£ã¦ã„ã‚‹ï¼
    pub(crate) strides: Vec<usize>,
    pub(crate) buffer: BufferHandle<T>,
    // ...
}

pub fn dims(&self) -> &[usize] {
    self.shape.dims()  // shapeãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ç›´æ¥å–å¾—
}
```

**å•é¡Œç‚¹**:
- `dims()`ã¯å®Ÿéš›ã«ã¯GPUåŒæœŸä¸è¦ï¼ˆshapeãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ãƒ¡ãƒ¢ãƒªä¸Šï¼‰
- ã—ã‹ã—ã€ä½•ã‚‰ã‹ã®ç†ç”±ã§åŒæœŸãŒç™ºç”Ÿã—ã¦ã„ã‚‹å¯èƒ½æ€§
- 1ãƒˆãƒ¼ã‚¯ãƒ³ã§22å›å‘¼ã³å‡ºã—

**æ€§èƒ½å½±éŸ¿**: 0.44-1.1ms/ãƒˆãƒ¼ã‚¯ãƒ³

#### ä¿®æ­£æ–¹æ³•

**Option A: shape()å‘¼ã³å‡ºã—æœ€é©åŒ–** (æ¨å¥¨)

TensorLogicã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§shapeå‘¼ã³å‡ºã—ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥:

```python
# Before (æ¯å±¤ã§shapeå–å¾—)
for layer in layers:
    let KV_shp = shape(KV)
    let cache_len = KV_shp[0]
    # ...

# After (ä¸€åº¦ã ã‘å–å¾—ã€å¤‰æ•°ã§è¿½è·¡)
let cache_len = initial_seq_len
for layer in layers:
    # cache_lenã‚’ç›´æ¥ä½¿ç”¨
    # KVè¿½åŠ å¾Œ: cache_len = cache_len + 1
```

**Option B: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿å†…ã§shapeã‚­ãƒ£ãƒƒã‚·ãƒ¥**

```rust
// Value enum ã«shapeã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¿½åŠ 
pub enum Value {
    TensorF32(Tensor<f32>),
    TensorF16(Tensor<f16>),
    // æ–°è¦è¿½åŠ 
    TensorShape(Vec<usize>),  // shapeå€¤å°‚ç”¨
}

fn eval_shape(&mut self, args: &[TensorExpr]) -> RuntimeResult<Value> {
    let val = self.eval_expr(&args[0])?;
    match val {
        Value::TensorF32(ref t) => {
            // GPUåŒæœŸãªã—ã§shapeå–å¾—
            let dims = t.shape.dims().to_vec();
            Ok(Value::TensorShape(dims))
        }
        Value::TensorF16(ref t) => {
            let dims = t.shape.dims().to_vec();
            Ok(Value::TensorShape(dims))
        }
        _ => Err(...)
    }
}
```

#### å®Ÿè£…æ‰‹é †

**ã‚¹ãƒ†ãƒƒãƒ—1**: ã‚¹ã‚¯ãƒªãƒ—ãƒˆæœ€é©åŒ–ï¼ˆå³åº§ã«å®Ÿæ–½å¯èƒ½ï¼‰

```bash
# examples/chat_full_22layers_f16.tl ã‚’ç·¨é›†
vim examples/chat_full_22layers_f16.tl

# å¤‰æ›´å†…å®¹:
# 1. cache_lenå¤‰æ•°ã‚’å°å…¥
# 2. å„å±¤ã§ã®shape()å‘¼ã³å‡ºã—ã‚’å‰Šé™¤
# 3. cache_len += 1 ã§æ›´æ–°
```

**ã‚¹ãƒ†ãƒƒãƒ—2**: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿æœ€é©åŒ–ï¼ˆä¸­æœŸï¼‰

```rust
// src/interpreter/value.rs
pub enum Value {
    // ...
    Shape(Vec<usize>),  // è¿½åŠ 
}

// src/interpreter/builtin_tensor.rs
fn eval_shape(&mut self, args: &[TensorExpr]) -> RuntimeResult<Value> {
    // GPUåŒæœŸãªã—ãƒãƒ¼ã‚¸ãƒ§ãƒ³å®Ÿè£…
}
```

#### ãƒªã‚¹ã‚¯è©•ä¾¡

| ãƒªã‚¹ã‚¯ | ç¢ºç‡ | å½±éŸ¿ | å¯¾ç­– |
|--------|------|------|------|
| cache_lenè¿½è·¡ãƒŸã‚¹ | ä½ | ä¸­ | assertionè¿½åŠ  |
| æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆäº’æ›æ€§ | ä½ | ä½ | å¾Œæ–¹äº’æ›æ€§ç¶­æŒ |

#### æœŸå¾…åŠ¹æœ

- **æ€§èƒ½æ”¹å–„**: 0.5-1ms/ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›
- **å‰¯æ¬¡åŠ¹æœ**: ã‚³ãƒ¼ãƒ‰å¯èª­æ€§å‘ä¸Šã€ãƒ­ã‚¸ãƒƒã‚¯ç°¡æ½”åŒ–

---

### å•é¡Œ3: O(NÂ²) KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†

#### ç¾çŠ¶åˆ†æ

**ç¾åœ¨ã®å®Ÿè£…** (`examples/chat_full_22layers_f16.tl`):

```python
# Prefill phase: Initial KV cache
let K0 = apply_rope_k(K0_raw, seq_len, 0.0)  # [seq_len, 256]
let V0 = linear(x, L0.attn_v.weight)         # [seq_len, 256]

# Decode phase (æ¯ãƒˆãƒ¼ã‚¯ãƒ³å®Ÿè¡Œ)
loop:
    # æ–°ã—ã„K/Vã‚’è¨ˆç®—
    let nK0 = apply_rope_k(nK0_raw, 1, cache_len)  # [1, 256]
    let nV0 = linear(x_new, L0.attn_v.weight)      # [1, 256]

    # concat() - æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡ã‚’å‰²ã‚Šå½“ã¦ã€å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼
    KV0 = concat(KV0, nK0, 0.0)     # [cache_len, 256] â†’ [cache_len+1, 256]
    KV0_V = concat(KV0_V, nV0, 0.0) # [cache_len, 256] â†’ [cache_len+1, 256]
```

**concat()ã®å‹•ä½œ** (`src/ops/tensor_ops.rs:68-165`):

```rust
pub fn concat(tensors: &[&Tensor<T>], dim: usize) -> TensorResult<Self> {
    // 1. å‡ºåŠ›ã‚µã‚¤ã‚ºè¨ˆç®—
    let mut output_shape = tensors[0].dims().to_vec();
    output_shape[dim] = tensors.iter().map(|t| t.dims()[dim]).sum();

    // 2. æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡å‰²ã‚Šå½“ã¦
    let output_buf = MetalBuffer::<T>::new_uninit(..., total_elements)?;

    // 3. å„å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
    for (i, tensor) in tensors.iter().enumerate() {
        // GPU kernelã§å€‹åˆ¥ã‚³ãƒ”ãƒ¼
        encoder.dispatch_threads(...);
    }

    // 4. æ–°ã—ã„ãƒ†ãƒ³ã‚½ãƒ«è¿”å´
    Tensor::new(BufferHandle::Metal(output_buf), output_shape, ...)
}
```

**å•é¡Œç‚¹**:

ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·Lã«å¯¾ã—ã¦ï¼š
- ãƒˆãƒ¼ã‚¯ãƒ³1: [1] (1è¦ç´ ã‚³ãƒ”ãƒ¼)
- ãƒˆãƒ¼ã‚¯ãƒ³2: [1,1] â†’ [2] (2è¦ç´ ã‚³ãƒ”ãƒ¼)
- ãƒˆãƒ¼ã‚¯ãƒ³3: [2,1] â†’ [3] (3è¦ç´ ã‚³ãƒ”ãƒ¼)
- ...
- ãƒˆãƒ¼ã‚¯ãƒ³L: [L-1,1] â†’ [L] (Lè¦ç´ ã‚³ãƒ”ãƒ¼)

**åˆè¨ˆ**: 1+2+3+...+L = **O(LÂ²)** ã‚³ãƒ”ãƒ¼æ“ä½œ

22å±¤ Ã— 2(K/V) = **44å›/ãƒˆãƒ¼ã‚¯ãƒ³** ã® concatæ“ä½œ

**æ€§èƒ½å½±éŸ¿**: 22-44ms/ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·34ã®å ´åˆï¼‰

#### ä¿®æ­£æ–¹æ³•

**Option A: äº‹å‰å‰²ã‚Šå½“ã¦ + In-placeæ›´æ–°** (æ¨å¥¨)

```python
# Prefill phase
let MAX_SEQ_LEN = 2048.0
let K0_cache = zeros([MAX_SEQ_LEN, 256])  # æœ€å¤§ã‚µã‚¤ã‚ºã‚’äº‹å‰å‰²ã‚Šå½“ã¦
let V0_cache = zeros([MAX_SEQ_LEN, 256])

# åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿
K0_cache = write_slice(K0_cache, K0, 0, seq_len)  # [0:seq_len] ã«æ›¸ãè¾¼ã¿
V0_cache = write_slice(V0_cache, V0, 0, seq_len)

let cache_len = seq_len

# Decode phase
loop:
    let nK0 = apply_rope_k(nK0_raw, 1, cache_len)
    let nV0 = linear(x_new, L0.attn_v.weight)

    # In-placeæ›´æ–°ï¼ˆæ–°ã—ã„ãƒãƒƒãƒ•ã‚¡å‰²ã‚Šå½“ã¦ãªã—ï¼‰
    K0_cache = write_slice(K0_cache, nK0, cache_len, cache_len + 1)
    V0_cache = write_slice(V0_cache, nV0, cache_len, cache_len + 1)

    cache_len = cache_len + 1

    # Attentionè¨ˆç®—æ™‚ã¯sliceã§ä½¿ç”¨ç¯„å›²æŒ‡å®š
    let K0_active = slice(K0_cache, 0, cache_len)  # [0:cache_len]
    let V0_active = slice(V0_cache, 0, cache_len)
```

**å¿…è¦ãªæ–°æ©Ÿèƒ½**:

1. **write_slice()**: æŒ‡å®šç¯„å›²ã«ãƒ†ãƒ³ã‚½ãƒ«ã‚’æ›¸ãè¾¼ã¿
2. **slice()**: æŒ‡å®šç¯„å›²ã®ãƒ“ãƒ¥ãƒ¼ã‚’ä½œæˆï¼ˆã‚³ãƒ”ãƒ¼ãªã—ï¼‰

**Option B: å‹•çš„æ‹¡å¼µãƒãƒƒãƒ•ã‚¡** (Rustã®`Vec`çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)

```python
# å®¹é‡ã‚’æŒã¤KVã‚­ãƒ£ãƒƒã‚·ãƒ¥
let K0_cache = reserve([256], 2048)  # shape=[0, 256], capacity=2048

# Appendæ“ä½œï¼ˆå®¹é‡å†…ãªã‚‰ã‚³ãƒ”ãƒ¼ãªã—ï¼‰
K0_cache = append(K0_cache, nK0)  # O(1) amortized
```

#### å®Ÿè£…æ‰‹é †

**ã‚¹ãƒ†ãƒƒãƒ—1**: write_sliceé–¢æ•°å®Ÿè£…

```rust
// src/interpreter/builtin_tensor.rs
fn eval_write_slice(&mut self, args: &[TensorExpr]) -> RuntimeResult<Value> {
    // args: [target_tensor, source_tensor, start_idx, end_idx]

    let target = self.eval_expr(&args[0])?;
    let source = self.eval_expr(&args[1])?;
    let start = self.extract_scalar(&args[2])?;
    let end = self.extract_scalar(&args[3])?;

    match (target, source) {
        (Value::TensorF16(mut t), Value::TensorF16(s)) => {
            // GPU kernelå‘¼ã³å‡ºã—: copy source â†’ target[start:end]
            t.write_slice_gpu(&s, start, end)?;
            Ok(Value::TensorF16(t))
        }
        // ...
    }
}
```

**ã‚¹ãƒ†ãƒƒãƒ—2**: GPU kernelå®Ÿè£…

```rust
// src/ops/tensor_ops.rs
impl<T: FloatType> Tensor<T> {
    pub fn write_slice_gpu(&mut self, source: &Tensor<T>, start: usize, end: usize) -> TensorResult<()> {
        // Metal kernel: memcpy with offset
        // shader: unified.metal ã« write_slice_f16/f32 è¿½åŠ 
    }
}
```

**ã‚¹ãƒ†ãƒƒãƒ—3**: ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›¸ãæ›ãˆ

```bash
# examples/chat_full_22layers_f16.tl
# concat() â†’ write_slice() ã«ç½®ãæ›ãˆ
```

**ã‚¹ãƒ†ãƒƒãƒ—4**: ãƒ†ã‚¹ãƒˆ

```bash
# å˜ä½“ãƒ†ã‚¹ãƒˆ
cargo test write_slice

# çµ±åˆãƒ†ã‚¹ãƒˆï¼ˆKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ãƒŠãƒªã‚ªï¼‰
./target/release/tl run tests/kv_cache_test.tl

# æ€§èƒ½æ¸¬å®š
time ./target/release/tl run examples/chat_full_22layers_f16.tl
```

#### ãƒªã‚¹ã‚¯è©•ä¾¡

| ãƒªã‚¹ã‚¯ | ç¢ºç‡ | å½±éŸ¿ | å¯¾ç­– |
|--------|------|------|------|
| ã‚¹ãƒ©ã‚¤ã‚¹å¢ƒç•Œã‚¨ãƒ©ãƒ¼ | ä¸­ | é«˜ | assertè¿½åŠ ã€ç¯„å›²ãƒã‚§ãƒƒã‚¯ |
| GPU kernel ãƒã‚° | ä¸­ | é«˜ | CPU fallbackã€æ®µéšçš„æ¤œè¨¼ |
| ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ | ä½ | ä¸­ | äº‹å‰å‰²ã‚Šå½“ã¦ã‚µã‚¤ã‚ºèª¿æ•´ |
| ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¤‡é›‘åŒ– | ä¸­ | ä½ | ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°è¿½åŠ  |

**ç·©å’Œç­–**:
- `write_slice`å®Ÿè£…ã«bounds check
- CPU fallback versionå®Ÿè£…
- ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ç¯„å›²assertion

#### æœŸå¾…åŠ¹æœ

- **æ€§èƒ½æ”¹å–„**: 20-40ms/ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«å¯¾ã—ã¦O(NÂ²) â†’ O(N)
- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: æ–­ç‰‡åŒ–å‰Šæ¸›ã€å‰²ã‚Šå½“ã¦å›æ•°æ¸›å°‘

---

### å•é¡Œ4: RoPEã§ã®ä¸è¦ãªã‚¯ãƒ­ãƒ¼ãƒ³

#### ç¾çŠ¶åˆ†æ

**å ´æ‰€**: `src/ops/rope.rs:45-55`

```rust
pub fn rope(&self, position_offset: usize) -> TensorResult<Self> {
    let is_contig = self.is_contiguous();
    if std::env::var("TL_DEBUG_ROPE").is_ok() {
        eprintln!("[ROPE] is_contiguous={}, will {}",
                 is_contig, if is_contig { "clone" } else { "make contiguous" });
    }

    let input = if is_contig {
        self.clone()  // â† å•é¡Œ: é€£ç¶šçš„ã§ã‚‚å¸¸ã«ã‚¯ãƒ­ãƒ¼ãƒ³ï¼
    } else {
        self.contiguous()?
    };

    // RoPEè¨ˆç®—...
}
```

**å•é¡Œç‚¹**:
- `is_contiguous() == true`ã§ã‚‚`clone()`å®Ÿè¡Œ
- RoPEå‘¼ã³å‡ºã—: Prefill 22å› + Decode 22å› = 44å›/ãƒˆãƒ¼ã‚¯ãƒ³
- å„`clone()`ã¯GPUåŒæœŸ + ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ + ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼

**æ€§èƒ½å½±éŸ¿**: 8.8-22ms/ãƒˆãƒ¼ã‚¯ãƒ³

#### ä¿®æ­£æ–¹æ³•

**Option A: In-place RoPE** (æœ€é©)

```rust
pub fn rope(&self, position_offset: usize) -> TensorResult<Self> {
    // contiguous checkã¯ä¸è¦ - GPU kernelãŒstrided tensorã‚’ç›´æ¥å‡¦ç†

    let (batch_sz, seq_len, n_heads, head_dim) = self.dims4()?;

    // æ–°ã—ã„ãƒãƒƒãƒ•ã‚¡å‰²ã‚Šå½“ã¦ï¼ˆcloneãªã—ï¼‰
    let output_buf = MetalBuffer::<T>::new_uninit_pooled(..., self.numel())?;

    // GPU kernel: input (stridedå¯) â†’ output (contiguous)
    self.rope_metal(output_buf, position_offset)?;

    Ok(Tensor::new(output_buf, self.shape().clone(), ...))
}
```

**Option B: æ¡ä»¶ä»˜ãã‚¯ãƒ­ãƒ¼ãƒ³** (ä¿å®ˆçš„)

```rust
pub fn rope(&self, position_offset: usize) -> TensorResult<Self> {
    // é€£ç¶šçš„ãªå ´åˆã¯ã‚¯ãƒ­ãƒ¼ãƒ³ã›ãšç›´æ¥å‡¦ç†
    if self.is_contiguous() {
        // in-placeå‡¦ç†ã¾ãŸã¯èª­ã¿å–ã‚Šå°‚ç”¨ã‚¢ã‚¯ã‚»ã‚¹
        self.rope_on_contiguous(position_offset)
    } else {
        // éé€£ç¶šçš„ãªå ´åˆã®ã¿contiguousåŒ–
        let contiguous = self.contiguous()?;
        contiguous.rope_on_contiguous(position_offset)
    }
}
```

#### å®Ÿè£…æ‰‹é †

**ã‚¹ãƒ†ãƒƒãƒ—1**: RoPE GPU kernelã‚’ç¢ºèª

```bash
# ã‚·ã‚§ãƒ¼ãƒ€ãƒ¼ãŒstrided tensorã‚’å‡¦ç†ã§ãã‚‹ã‹ç¢ºèª
cat shaders/unified.metal | grep -A 50 "kernel void rope"
```

**ã‚¹ãƒ†ãƒƒãƒ—2**: clone()å‰Šé™¤

```rust
// src/ops/rope.rs
pub fn rope(&self, position_offset: usize) -> TensorResult<Self> {
    // å‰Šé™¤: let input = if is_contig { self.clone() } else { self.contiguous()? };

    // ç›´æ¥å‡¦ç†
    let output = self.rope_metal_direct(position_offset)?;
    Ok(output)
}
```

**ã‚¹ãƒ†ãƒƒãƒ—3**: ãƒ†ã‚¹ãƒˆ

```bash
# RoPEãƒ†ã‚¹ãƒˆ
cargo test rope

# æ•°å€¤ç²¾åº¦ãƒ†ã‚¹ãƒˆï¼ˆcloneå‰Šé™¤å¾Œã‚‚åŒã˜çµæœã‹ï¼‰
TL_DEBUG_ROPE=1 ./target/release/tl run tests/rope_precision.tl
```

#### ãƒªã‚¹ã‚¯è©•ä¾¡

| ãƒªã‚¹ã‚¯ | ç¢ºç‡ | å½±éŸ¿ | å¯¾ç­– |
|--------|------|------|------|
| Strided tensoréå¯¾å¿œ | ä¸­ | é«˜ | kernelã§å¯¾å¿œã€ã¾ãŸã¯contiguousåŒ– |
| æ•°å€¤ç²¾åº¦å¤‰åŒ– | ä½ | ä¸­ | å›å¸°ãƒ†ã‚¹ãƒˆ |

#### æœŸå¾…åŠ¹æœ

- **æ€§èƒ½æ”¹å–„**: 8-20ms/ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›
- **ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: 44å›ã®ã‚¯ãƒ­ãƒ¼ãƒ³å‰Šæ¸›

---

### å•é¡Œ5: ã‚³ãƒãƒ³ãƒ‰ãƒãƒƒãƒ•ã‚¡ãƒãƒƒãƒã‚µã‚¤ã‚º

#### ç¾çŠ¶åˆ†æ

**å ´æ‰€**: `src/device/commands.rs:89-102`

```rust
pub fn command_buffer(&self) -> Result<(bool, Arc<CommandBuffer>), DeviceError> {
    let mut command_buffers = self.command_buffers.lock().unwrap();
    let (command_buffer, flushed) = command_buffers.entry(thread_id()).or_insert_with(|| {
        // ...
    });

    self.command_buffer_index += 1;

    // Check if we need to flush (exceeded batch size)
    if self.command_buffer_index > self.compute_per_buffer {  // â† 50æ“ä½œã§ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
        command_buffer.commit();
        // æ–°ã—ã„command bufferä½œæˆ...
        self.command_buffer_index = 0;
        flushed = true;
    }

    Ok((flushed, Arc::clone(command_buffer)))
}
```

**è¨­å®š**: `compute_per_buffer = 50` (å°ã•ã„)

**å•é¡Œç‚¹**:
- 1ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ†ãƒƒãƒ—ã§ç´„200+æ“ä½œ
- 200 / 50 = 4å›ã®commit
- å„commitæ™‚ã«GPUå®Ÿè¡Œé–‹å§‹ï¼ˆç´°åˆ‡ã‚Œå®Ÿè¡Œï¼‰

**æ€§èƒ½å½±éŸ¿**: 2-6ms/ãƒˆãƒ¼ã‚¯ãƒ³

#### ä¿®æ­£æ–¹æ³•

**Option A: ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—åŠ ** (å³åº§)

```rust
// src/device/metal_device.rs ã¾ãŸã¯ commands.rs
pub fn new(...) -> Self {
    Self {
        // Before: compute_per_buffer: 50
        compute_per_buffer: 500,  // 10å€ã«å¢—åŠ 
        // ã¾ãŸã¯
        compute_per_buffer: 1000,  // 20å€ã«å¢—åŠ 
        // ...
    }
}
```

**Option B: æ‰‹å‹•ãƒ•ãƒ©ãƒƒã‚·ãƒ¥åˆ¶å¾¡** (æŸ”è»Ÿ)

```rust
// è‡ªå‹•ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–
pub fn disable_auto_flush(&mut self) {
    self.auto_flush_enabled = false;
}

pub fn manual_flush(&mut self) -> Result<(), DeviceError> {
    // ç¾åœ¨ã®command bufferã‚’æ˜ç¤ºçš„ã«commit
    // ...
}

// ä½¿ç”¨ä¾‹ï¼ˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿å±¤ã‹ã‚‰ï¼‰
device.disable_auto_flush();
// ... 1å±¤åˆ†ã®è¨ˆç®— ...
device.manual_flush();  // å±¤å˜ä½ã§ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
```

#### å®Ÿè£…æ‰‹é †

**ã‚¹ãƒ†ãƒƒãƒ—1**: ãƒãƒƒãƒã‚µã‚¤ã‚ºå¤‰æ›´

```rust
// src/device/metal_device.rs (ã¾ãŸã¯è©²å½“ãƒ•ã‚¡ã‚¤ãƒ«)
// compute_per_bufferã‚’æ¢ã—ã¦å¤‰æ›´
compute_per_buffer: 500  // ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã§è¨­å®š
```

**ã‚¹ãƒ†ãƒƒãƒ—2**: ç’°å¢ƒå¤‰æ•°å¯¾å¿œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

```rust
let batch_size = std::env::var("TL_COMMAND_BATCH_SIZE")
    .ok()
    .and_then(|s| s.parse::<usize>().ok())
    .unwrap_or(500);  // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ500

Self {
    compute_per_buffer: batch_size,
    // ...
}
```

**ã‚¹ãƒ†ãƒƒãƒ—3**: ãƒ†ã‚¹ãƒˆ

```bash
# æ€§èƒ½æ¸¬å®š
TL_COMMAND_BATCH_SIZE=1000 time ./target/release/tl run examples/chat_full_22layers_f16.tl

# å®‰å®šæ€§ãƒ†ã‚¹ãƒˆï¼ˆå¤§ãã™ãã‚‹ã¨ãƒ¡ãƒ¢ãƒªå•é¡Œã®å¯èƒ½æ€§ï¼‰
for size in 100 500 1000 2000; do
    TL_COMMAND_BATCH_SIZE=$size ./target/release/tl run tests/stress_test.tl
done
```

#### ãƒªã‚¹ã‚¯è©•ä¾¡

| ãƒªã‚¹ã‚¯ | ç¢ºç‡ | å½±éŸ¿ | å¯¾ç­– |
|--------|------|------|------|
| ãƒ¡ãƒ¢ãƒªåœ§è¿« | ä½ | ä¸­ | æ®µéšçš„å¢—åŠ ã€ç›£è¦– |
| ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¢—åŠ  | ä½ | ä½ | æ¸¬å®šã€èª¿æ•´ |

#### æœŸå¾…åŠ¹æœ

- **æ€§èƒ½æ”¹å–„**: 2-6ms/ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›
- **GPUåˆ©ç”¨ç‡å‘ä¸Š**: ã‚ˆã‚Šå¤§ããªå˜ä½ã§å®Ÿè¡Œ

---

## Phase 2: ä¸­æœŸæ”¹å–„ (95%æ€§èƒ½å‘ä¸Š)

### è¿½åŠ æœ€é©åŒ–é …ç›®

1. **é…å»¶contiguousè©•ä¾¡**
   - `broadcast()`, `transpose()`ã®çµæœã‚’é…å»¶è©•ä¾¡
   - å®Ÿéš›ã«å¿…è¦ã«ãªã‚‹ã¾ã§contiguousåŒ–ã—ãªã„

2. **æ“ä½œèåˆ**
   - matmul + ReLU/SiLU ã‚’1ã¤ã®kernelã«
   - LayerNorm + ç·šå½¢æŠ•å½±ã®èåˆ

3. **ãƒãƒƒãƒ•ã‚¡ãƒ—ãƒ¼ãƒ«æœ€é©åŒ–**
   - ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ¼ãƒ«
   - ã‚µã‚¤ã‚ºåˆ¥ãƒ—ãƒ¼ãƒ«ç®¡ç†

---

## Phase 3: é•·æœŸæ”¹å–„ (CandleåŒç­‰)

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´

1. **é…å»¶å®Ÿè¡Œã‚°ãƒ©ãƒ•**
   - è¨ˆç®—ã‚°ãƒ©ãƒ•æ§‹ç¯‰
   - è‡ªå‹•æœ€é©åŒ–ãƒ‘ã‚¹

2. **Metal Performance Shaders (MPS)**
   - Appleã®æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ´»ç”¨
   - é«˜æ€§èƒ½ãªmatmul/convå®Ÿè£…

3. **è‡ªå‹•ã‚«ãƒ¼ãƒãƒ«é¸æŠ**
   - å…¥åŠ›ã‚µã‚¤ã‚ºã«å¿œã˜ãŸæœ€é©kernelé¸æŠ
   - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ™ãƒ¼ã‚¹ã®auto-tuning

---

## å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

### Week 1: Phase 1 å®Ÿè£…
- Day 1-2: å•é¡Œ1 (tensorä½œæˆåŒæœŸå‰Šé™¤)
- Day 3: å•é¡Œ2 (shapeã‚­ãƒ£ãƒƒã‚·ãƒ¥)
- Day 4-5: å•é¡Œ3 (KVã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–)

### Week 2: Phase 1 å®Œäº†
- Day 1-2: å•é¡Œ4 (RoPEã‚¯ãƒ­ãƒ¼ãƒ³å‰Šé™¤)
- Day 3: å•é¡Œ5 (ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´)
- Day 4-5: çµ±åˆãƒ†ã‚¹ãƒˆã€æ€§èƒ½æ¸¬å®š

### Week 3-4: Phase 2
- é…å»¶è©•ä¾¡ã€æ“ä½œèåˆ

### Month 2-3: Phase 3
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´

---

## ãƒ†ã‚¹ãƒˆè¨ˆç”»

### æ­£ç¢ºæ€§ãƒ†ã‚¹ãƒˆ
```bash
# æ—¢å­˜ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
cargo test --release

# æ•°å€¤ç²¾åº¦ãƒ†ã‚¹ãƒˆ
./target/release/tl run tests/numerical_precision.tl

# Candleå‡ºåŠ›ã¨ã®æ¯”è¼ƒ
./scripts/compare_with_candle.sh
```

### æ€§èƒ½ãƒ†ã‚¹ãƒˆ
```bash
# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
hyperfine './target/release/tl run examples/chat_full_22layers_f16.tl'

# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
instruments -t "Time Profiler" ./target/release/tl run examples/chat_full_22layers_f16.tl

# GPUåˆ©ç”¨ç‡
instruments -t "GPU" ./target/release/tl run examples/chat_full_22layers_f16.tl
```

### å›å¸°ãƒ†ã‚¹ãƒˆ
```bash
# å„ä¿®æ­£å¾Œã«å®Ÿè¡Œ
./scripts/run_all_tests.sh

# CI/CDçµ±åˆ
# - Pushæ™‚ã«è‡ªå‹•ãƒ†ã‚¹ãƒˆ
# - æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨˜éŒ²
```

---

## æ€§èƒ½è¿½è·¡

### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (ä¿®æ­£å‰)
- **Decodeé€Ÿåº¦**: 0.011 tok/s (90ç§’/ãƒˆãƒ¼ã‚¯ãƒ³)
- **Prefillé€Ÿåº¦**: ~5ç§’ (34ãƒˆãƒ¼ã‚¯ãƒ³)

### ç›®æ¨™

| Phase | ç›®æ¨™é€Ÿåº¦ | æ”¹å–„ç‡ | é”æˆæ™‚æœŸ |
|-------|---------|--------|---------|
| Phase 1 | 55 tok/s | 5000å€ | Week 2 |
| Phase 2 | 125 tok/s | 11,000å€ | Week 4 |
| Phase 3 | 200+ tok/s | 18,000å€ | Month 3 |

---

## ã¾ã¨ã‚

### é‡è¦ç™ºè¦‹
1. æ€§èƒ½å•é¡Œã®95%ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆè¨ˆç®—ã§ã¯ãªã„ï¼‰
2. æœ€å¤§ã®å•é¡Œ: 1,100+å›/ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸è¦ãªGPUåŒæœŸ
3. ä¿®æ­£ã¯æ¯”è¼ƒçš„straightforward

### æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
1. å•é¡Œ1ï¼ˆtensorä½œæˆåŒæœŸï¼‰ã‹ã‚‰ç€æ‰‹ â†’ æœ€å¤§ã®åŠ¹æœ
2. å„ä¿®æ­£ã‚’ç‹¬ç«‹ã—ã¦ãƒ†ã‚¹ãƒˆ
3. æ®µéšçš„ã«rollout

### æœŸå¾…ã•ã‚Œã‚‹çµæœ
- **Phase 1å®Œäº†å¾Œ**: Candleã®1/7ç¨‹åº¦ã®æ€§èƒ½ï¼ˆå®Ÿç”¨ãƒ¬ãƒ™ãƒ«ï¼‰
- **Phase 2å®Œäº†å¾Œ**: Candleã®1/3ç¨‹åº¦ã®æ€§èƒ½
- **Phase 3å®Œäº†å¾Œ**: CandleåŒç­‰ã®æ€§èƒ½

---

**ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ç”ŸããŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚å®Ÿè£…ã®é€²æ—ã«å¿œã˜ã¦æ›´æ–°ã—ã¦ãã ã•ã„ã€‚**
